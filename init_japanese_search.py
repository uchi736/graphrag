"""
æ—¥æœ¬èªæ¤œç´¢ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¸ã® tokenized_content è¿½åŠ 
"""
import os
import sys
import psycopg
from dotenv import load_dotenv
from japanese_text_processor import get_japanese_processor

load_dotenv()
PG_CONN = os.getenv("PG_CONN")

if not PG_CONN:
    print("âŒ ã‚¨ãƒ©ãƒ¼: PG_CONN ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    sys.exit(1)


def init_db_schema():
    """ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒã‚’åˆæœŸåŒ–ã—ã¦ã„ã¾ã™...")
    try:
        with psycopg.connect(PG_CONN) as conn:
            with conn.cursor() as cur:
                # åˆ—è¿½åŠ 
                cur.execute("""
                    ALTER TABLE langchain_pg_embedding
                    ADD COLUMN IF NOT EXISTS tokenized_content TEXT
                """)

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                cur.execute("""
                    CREATE INDEX IF NOT EXISTS idx_embedding_tokenized_gin
                    ON langchain_pg_embedding
                    USING gin (to_tsvector('simple', COALESCE(tokenized_content, '')))
                """)
            conn.commit()
        print("âœ… ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–å®Œäº†")
        return True
    except Exception as e:
        print(f"âŒ ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def migrate_existing_data():
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–è¿½åŠ """
    processor = get_japanese_processor()
    if not processor:
        print("âŒ SudachiãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        print("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install sudachipy sudachidict_core")
        return False

    print("ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œã—ã¦ã„ã¾ã™...")
    try:
        with psycopg.connect(PG_CONN) as conn:
            with conn.cursor() as cur:
                # tokenized_contentãŒNULLã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
                cur.execute("""
                    SELECT id, document
                    FROM langchain_pg_embedding
                    WHERE tokenized_content IS NULL
                """)

                rows = cur.fetchall()
                total = len(rows)

                if total == 0:
                    print("âœ… ç§»è¡Œå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆæ—¢ã«ç§»è¡Œæ¸ˆã¿ï¼‰")
                    return True

                print(f"ğŸ“Š {total}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‡¦ç†ã—ã¾ã™...")

                for idx, (record_id, text) in enumerate(rows, 1):
                    if idx % 10 == 0 or idx == total:
                        print(f"  å‡¦ç†ä¸­: {idx}/{total} ({idx*100//total}%)")

                    tokenized = processor.tokenize(text)
                    cur.execute("""
                        UPDATE langchain_pg_embedding
                        SET tokenized_content = %s
                        WHERE id = %s
                    """, (tokenized, record_id))

            conn.commit()
        print("âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œå®Œäº†")
        return True
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return False


def verify_setup():
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª"""
    print("\nğŸ“Š ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ç¢ºèªã—ã¦ã„ã¾ã™...")
    try:
        with psycopg.connect(PG_CONN) as conn:
            with conn.cursor() as cur:
                # tokenized_contentåˆ—ã®å­˜åœ¨ç¢ºèª
                cur.execute("""
                    SELECT column_name
                    FROM information_schema.columns
                    WHERE table_name = 'langchain_pg_embedding'
                      AND column_name = 'tokenized_content'
                """)
                if cur.fetchone():
                    print("  âœ… tokenized_contentåˆ—: å­˜åœ¨")
                else:
                    print("  âŒ tokenized_contentåˆ—: å­˜åœ¨ã—ãªã„")
                    return False

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å­˜åœ¨ç¢ºèª
                cur.execute("""
                    SELECT indexname
                    FROM pg_indexes
                    WHERE tablename = 'langchain_pg_embedding'
                      AND indexname = 'idx_embedding_tokenized_gin'
                """)
                if cur.fetchone():
                    print("  âœ… GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: å­˜åœ¨")
                else:
                    print("  âŒ GINã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: å­˜åœ¨ã—ãªã„")
                    return False

                # ãƒ‡ãƒ¼ã‚¿ä»¶æ•°ç¢ºèª
                cur.execute("""
                    SELECT
                        COUNT(*) as total,
                        COUNT(tokenized_content) as tokenized
                    FROM langchain_pg_embedding
                """)
                row = cur.fetchone()
                if row:
                    total, tokenized = row
                    print(f"  ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total}")
                    print(f"  ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³åŒ–æ¸ˆã¿: {tokenized} ({tokenized*100//total if total > 0 else 0}%)")

        print("\nâœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯æ­£å¸¸ã§ã™")
        return True
    except Exception as e:
        print(f"\nâŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("æ—¥æœ¬èªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    print()

    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¹ã‚­ãƒ¼ãƒåˆæœŸåŒ–
    if not init_db_schema():
        print("\nâŒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)

    print()

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‡ãƒ¼ã‚¿ç§»è¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    migrate = input("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œã—ã¾ã™ã‹ï¼Ÿ [y/N]: ").strip().lower()
    if migrate == 'y':
        print()
        if not migrate_existing_data():
            print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿ç§»è¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")
            print("   â€» ã‚¹ã‚­ãƒ¼ãƒã¯åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€æ–°è¦ãƒ‡ãƒ¼ã‚¿ã¯æ­£å¸¸ã«å‡¦ç†ã•ã‚Œã¾ã™")

    print()

    # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª
    verify_setup()

    print()
    print("=" * 60)
    print("åˆæœŸåŒ–å®Œäº†")
    print("=" * 60)
    print()
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. Sudachiã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install sudachipy sudachidict_core")
    print("  2. .envã« ENABLE_JAPANESE_SEARCH=true ã‚’è¿½åŠ ")
    print("  3. ã‚¢ãƒ—ãƒªã‚’èµ·å‹•: streamlit run app.py")
    print()
