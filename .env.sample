# Graph Backend Configuration
# グラフデータベースのバックエンド選択
# "neo4j" = Neo4j Aura/Server (推奨: 大規模データ、高度なクエリ)
# "networkx" = NetworkX (推奨: 軽量、Neo4j不要、小〜中規模データ)
GRAPH_BACKEND=networkx

# Neo4j Configuration (GRAPH_BACKEND=neo4j の場合のみ必要)
# Neo4j Auraの接続情報
# 例: neo4j+s://xxxxx.databases.neo4j.io
NEO4J_URI=neo4j+s://your-instance-id.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PW=your_neo4j_password

# PostgreSQL Configuration
# PGVector拡張がインストールされたPostgreSQLの接続文字列
# 例: postgresql+psycopg://user:password@host:port/database
PG_CONN=postgresql+psycopg://postgres:your_password@your-host:5432/postgres

# Azure OpenAI Service Settings
# Azure OpenAIの接続情報
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4o
AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-small

# Japanese Hybrid Search Configuration
# 日本語ハイブリッド検索の設定
# ベクトル検索とキーワード検索をRRFで統合（精度向上）
ENABLE_JAPANESE_SEARCH=true
# 最小トークン長（1文字のトークンを除外）
JAPANESE_MIN_TOKEN_LENGTH=2

# Retrieval Configuration
# 検索設定
# RAG検索で取得するチャンク数（1-20、デフォルト: 5）
# 多いほど文脈が豊富になりますが、処理時間が増加します
RETRIEVAL_TOP_K=5

# LLM Provider Configuration
# LLMプロバイダー設定
# "azure_openai" = Azure OpenAI Service (デフォルト)
# "vllm" = VLLM Server (セルフホステッドモデル)
LLM_PROVIDER=azure_openai

# VLLM Configuration (LLM_PROVIDER=vllm の場合のみ必要)
# VLLMサーバーの設定
# エンドポイントURL（末尾に /v1 を含める）
VLLM_ENDPOINT=https://your-vllm-server.example.com/v1
# 生成パラメータ
VLLM_TEMPERATURE=0.0
VLLM_TOP_P=0.7
VLLM_TOP_K=5
VLLM_MIN_P=0.0
VLLM_MAX_TOKENS=4096
VLLM_REASONING_EFFORT=medium
# タイムアウト設定（秒）
VLLM_TIMEOUT=60
