#!/usr/bin/env python
"""
build_kg.py - CLIç‰ˆãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
================================================
Streamlitã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã®CLIãƒ„ãƒ¼ãƒ«ã€‚
ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šã§è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†ã—ã€graph.pkl/graph.jsonã«ä¿å­˜ã€‚

ä½¿ç”¨ä¾‹:
    # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    python build_kg.py --input ./docs

    # æ–°è¦æ§‹ç¯‰ï¼ˆå‡¦ç†æ¸ˆã¿ã‚’ã‚¯ãƒªã‚¢ï¼‰
    python build_kg.py --input ./docs --fresh

    # ç‰¹å®šã®æ‹¡å¼µå­ã®ã¿
    python build_kg.py --input ./docs --ext pdf,md
"""

import argparse
import hashlib
import os
import sys
from pathlib import Path
from typing import List, Optional

from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from chunk_utils import create_markdown_chunks
from tqdm import tqdm

from prompt import KG_SYSTEM_PROMPT, KG_USER_PROMPT

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()


def load_file(file_path: Path) -> Optional[Document]:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§Documentã‚’è¿”ã™"""
    try:
        suffix = file_path.suffix.lower()
        file_name = file_path.name

        if suffix == '.pdf':
            # Azure Document Intelligence ã¾ãŸã¯ PyMuPDF
            azure_di_endpoint = os.getenv("AZURE_DI_ENDPOINT")
            azure_di_api_key = os.getenv("AZURE_DI_API_KEY")

            if azure_di_endpoint and azure_di_api_key:
                try:
                    from azure_di_processor import AzureDocumentIntelligenceProcessor, AzureDIConfig
                    config = AzureDIConfig()
                    processor = AzureDocumentIntelligenceProcessor(config)
                    docs = processor.process(str(file_path))
                    if docs:
                        text_content = docs[0].page_content

                        # Azure DIå‡ºåŠ›ã‚’ä¿å­˜
                        output_dir = Path("output")
                        output_dir.mkdir(exist_ok=True)
                        output_filename = file_path.stem + "_azure_di.md"
                        output_path = output_dir / output_filename
                        with open(output_path, "w", encoding="utf-8") as f:
                            f.write(f"# {file_name}\n\n")
                            f.write(f"*Processed by Azure Document Intelligence*\n\n")
                            f.write("---\n\n")
                            f.write(text_content)
                        print(f"  ğŸ“„ Azure DIå‡ºåŠ›ã‚’ä¿å­˜: {output_path}")
                    else:
                        text_content = ""
                except Exception as e:
                    print(f"  âš ï¸ Azure DIå‡¦ç†ã‚¨ãƒ©ãƒ¼ã€PyMuPDFã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
                    import fitz
                    pdf_doc = fitz.open(str(file_path))
                    text_parts = []
                    for page_num in range(len(pdf_doc)):
                        page = pdf_doc[page_num]
                        text = page.get_text("text", sort=True)
                        if text.strip():
                            text_parts.append(text)
                    pdf_doc.close()
                    text_content = "\n\n".join(text_parts)
            else:
                # PyMuPDF
                import fitz
                pdf_doc = fitz.open(str(file_path))
                text_parts = []
                for page_num in range(len(pdf_doc)):
                    page = pdf_doc[page_num]
                    text = page.get_text("text", sort=True)
                    if text.strip():
                        text_parts.append(text)
                pdf_doc.close()
                text_content = "\n\n".join(text_parts)

        elif suffix in ['.txt', '.md']:
            with open(file_path, 'r', encoding='utf-8') as f:
                text_content = f.read()
            if suffix == '.md' and file_name.endswith('_azure_di.md'):
                print(f"  ğŸ“„ Azure DIå‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿")
        else:
            print(f"  âš ï¸ æœªå¯¾å¿œã®æ‹¡å¼µå­: {suffix}")
            return None

        if not text_content.strip():
            print(f"  âš ï¸ ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«")
            return None

        return Document(
            page_content=text_content,
            metadata={"source": file_name}
        )

    except Exception as e:
        print(f"  âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def build_knowledge_graph(
    input_dir: Path,
    extensions: List[str],
    fresh: bool = False
):
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰"""
    from langchain_experimental.graph_transformers import LLMGraphTransformer
    from llm_factory import create_chat_llm
    from networkx_graph import NetworkXGraph

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
    files = []
    for ext in extensions:
        files.extend(input_dir.glob(f"**/*.{ext}"))
    files = sorted(set(files))

    if not files:
        print(f"âŒ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_dir}")
        print(f"   æ‹¡å¼µå­: {extensions}")
        return

    print(f"\nğŸ“ å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€: {input_dir}")
    print(f"ğŸ“„ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
    print(f"   æ‹¡å¼µå­: {', '.join(extensions)}")

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print(f"\n{'='*50}")
    print("ğŸ“– ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    print(f"{'='*50}")

    source_docs = []
    for file_path in tqdm(files, desc="èª­ã¿è¾¼ã¿"):
        print(f"\n  {file_path.name}")
        doc = load_file(file_path)
        if doc:
            source_docs.append(doc)

    if not source_docs:
        print("âŒ èª­ã¿è¾¼ã‚ãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    total_chars = sum(len(doc.page_content) for doc in source_docs)
    print(f"\nâœ… {len(source_docs)}ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆç·æ–‡å­—æ•°: {total_chars:,}æ–‡å­—ï¼‰")

    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
    print(f"\n{'='*50}")
    print("âœ‚ï¸ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸­...")
    print(f"{'='*50}")

    # 2æ®µéšMarkdownãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ï¼ˆ##, ### ã§åˆ†å‰² â†’ 1024æ–‡å­—ã§å†åˆ†å‰²ï¼‰
    all_chunks = create_markdown_chunks(source_docs, chunk_size=1024, chunk_overlap=100)

    # é‡è¤‡é™¤å»
    deduped = []
    seen_hashes = set()
    for chunk in all_chunks:
        digest = hashlib.sha256(chunk.page_content.encode("utf-8")).hexdigest()
        if digest in seen_hashes:
            continue
        seen_hashes.add(digest)
        chunk.metadata["id"] = digest
        deduped.append(chunk)

    chunks = deduped
    print(f"âœ… {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")

    # ã‚°ãƒ©ãƒ•åˆæœŸåŒ–
    print(f"\n{'='*50}")
    print("ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ä¸­...")
    print(f"{'='*50}")

    graph = NetworkXGraph(storage_path="graph.pkl", auto_save=False)

    # æ–°è¦æ§‹ç¯‰ã®å ´åˆã¯å‡¦ç†æ¸ˆã¿ã‚¯ãƒªã‚¢
    if fresh:
        graph.clear_processed_hashes()
        print("ğŸ—‘ï¸ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

    # å‡¦ç†æ¸ˆã¿ãƒãƒƒã‚·ãƒ¥å–å¾—
    processed_hashes = graph.get_processed_hashes()

    # æœªå‡¦ç†ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚£ãƒ«ã‚¿
    pending_chunks = [c for c in chunks if c.metadata.get("id") not in processed_hashes]
    skipped_count = len(chunks) - len(pending_chunks)

    if skipped_count > 0:
        print(f"ğŸ“‹ å‡¦ç†å¯¾è±¡: {len(pending_chunks)}/{len(chunks)}ãƒãƒ£ãƒ³ã‚¯ï¼ˆ{skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    if not pending_chunks:
        print("âœ… ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã¯å‡¦ç†æ¸ˆã¿ã§ã™")
        return

    # LLMGraphTransformerè¨­å®š
    llm_provider = os.getenv("LLM_PROVIDER", "azure_openai").lower()
    print(f"ğŸ¤– LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {llm_provider}")

    if llm_provider == "vllm":
        llm = create_chat_llm(temperature=0)
        transformer = LLMGraphTransformer(
            llm=llm,
            allowed_nodes=["Term"],
            allowed_relationships=[
                "IS_A", "BELONGS_TO_CATEGORY", "PART_OF", "HAS_STEP",
                "HAS_ATTRIBUTE", "RELATED_TO", "AFFECTS", "CAUSES",
                "DEPENDS_ON", "APPLIES_TO", "OWNED_BY", "SAME_AS"
            ],
            strict_mode=False,
        )
    else:
        from langchain_core.prompts import ChatPromptTemplate
        llm = create_chat_llm(temperature=0)

        kg_prompt = ChatPromptTemplate.from_messages([
            ("system", KG_SYSTEM_PROMPT),
            ("user", KG_USER_PROMPT)
        ])

        transformer = LLMGraphTransformer(
            llm=llm,
            prompt=kg_prompt,
            allowed_nodes=["Term"],
            allowed_relationships=[
                "IS_A", "BELONGS_TO_CATEGORY", "PART_OF", "HAS_STEP",
                "HAS_ATTRIBUTE", "RELATED_TO", "AFFECTS", "CAUSES",
                "DEPENDS_ON", "APPLIES_TO", "OWNED_BY", "SAME_AS"
            ],
            strict_mode=False,
        )

    # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å‡¦ç†ï¼ˆ100ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«å®šæœŸä¿å­˜ï¼‰
    SAVE_INTERVAL = 100
    success_count = 0
    error_count = 0

    for chunk in tqdm(pending_chunks, desc="ã‚°ãƒ©ãƒ•æ§‹ç¯‰"):
        try:
            chunk_docs = transformer.convert_to_graph_documents([chunk])
            graph.add_graph_documents(chunk_docs, include_source=True)

            chunk_hash = chunk.metadata.get("id")
            if chunk_hash:
                graph.mark_chunk_processed(chunk_hash, save=False)

            success_count += 1

            # å®šæœŸä¿å­˜ï¼ˆé€²æ—ã‚’å¤±ã‚ãªã„ãŸã‚ï¼‰
            if success_count % SAVE_INTERVAL == 0:
                graph.save()

        except Exception as e:
            error_count += 1
            tqdm.write(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼: {e}")
            graph.save()
            continue

    # æœ€çµ‚ä¿å­˜
    graph.save()

    # ã‚°ãƒ©ãƒ•çµ±è¨ˆ
    node_count = graph.graph.number_of_nodes()
    edge_count = graph.graph.number_of_edges()
    print(f"\nâœ… ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†: æˆåŠŸ{success_count}ä»¶, ã‚¨ãƒ©ãƒ¼{error_count}ä»¶")
    print(f"   ãƒãƒ¼ãƒ‰æ•°: {node_count}, ã‚¨ãƒƒã‚¸æ•°: {edge_count}")

    # PGVectorä¿å­˜
    print(f"\n{'='*50}")
    print("ğŸ“¦ PGVectorä¿å­˜ä¸­...")
    print(f"{'='*50}")

    try:
        from langchain_openai import AzureOpenAIEmbeddings
        from db_utils import ensure_embedding_id_unique, ensure_schema_compatibility, ensure_hnsw_index, add_connection_timeout, batch_pgvector_from_documents

        PG_CONN = os.getenv("PG_CONN")
        PG_COLLECTION = os.getenv("PG_COLLECTION", "graphrag")

        if not PG_CONN:
            print("âš ï¸ PG_CONNæœªè¨­å®šã®ãŸã‚PGVectorä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        else:
            embeddings = AzureOpenAIEmbeddings(
                azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME"),
                openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                api_key=os.getenv("AZURE_OPENAI_API_KEY")
            )

            # IDã®NULLãƒã‚§ãƒƒã‚¯
            ids = [c.metadata.get("id") for c in chunks if c.metadata.get("id")]
            if len(ids) != len(chunks):
                print("âš ï¸ ä¸€éƒ¨ãƒãƒ£ãƒ³ã‚¯ã«IDãŒã‚ã‚Šã¾ã›ã‚“")

            ensure_embedding_id_unique(PG_CONN)
            ensure_schema_compatibility(PG_CONN)
            ensure_hnsw_index(PG_CONN)

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
            pg_conn_with_timeout = add_connection_timeout(PG_CONN, timeout=30)

            # ãƒãƒƒãƒåˆ†å‰²ã§PGVectorä¿å­˜ï¼ˆbindãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸Šé™å¯¾ç­–ï¼‰
            vector_store = batch_pgvector_from_documents(
                chunks,
                embeddings,
                connection=pg_conn_with_timeout,
                collection_name=PG_COLLECTION,
                pre_delete_collection=fresh,  # æ–°è¦æ§‹ç¯‰æ™‚ã®ã¿å‰Šé™¤
                progress_callback=lambda i, total, n: print(f"  PGVector: {i+n}/{total}ãƒãƒ£ãƒ³ã‚¯"),
            )
            print(f"âœ… PGVectorä¿å­˜å®Œäº†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯")

            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            print(f"\n{'='*50}")
            print("ğŸ” ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­...")
            print(f"{'='*50}")

            try:
                from entity_vectorizer import EntityVectorizer

                entity_vectorizer = EntityVectorizer(PG_CONN, embeddings)

                # ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
                entities = entity_vectorizer.extract_entities_from_graph(
                    graph,
                    graph_backend="networkx"
                )
                print(f"  æŠ½å‡ºã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ•°: {len(entities)}")

                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜
                # graph_docsã¯ç©ºã§ã‚‚OKï¼ˆã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£IDã ã‘ä¿å­˜ï¼‰
                num_saved = entity_vectorizer.add_entities(entities, [])

                if num_saved > 0:
                    print(f"âœ… {num_saved}å€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã—ãŸ")
                elif len(entities) == 0:
                    print("âš ï¸ ã‚°ãƒ©ãƒ•ã«ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                else:
                    print(f"âš ï¸ {len(entities)}å€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

            except ImportError:
                print("âš ï¸ EntityVectorizerãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    except Exception as e:
        print(f"âš ï¸ PGVectorä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # çµæœè¡¨ç¤º
    print(f"\n{'='*50}")
    print("ğŸ“Š æœ€çµ‚çµæœ")
    print(f"{'='*50}")
    print(f"ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•: graph.pkl, graph.json")
    print(f"   ãƒãƒ¼ãƒ‰æ•°: {node_count}")
    print(f"   ã‚¨ãƒƒã‚¸æ•°: {edge_count}")


def main():
    parser = argparse.ArgumentParser(
        description="CLIç‰ˆãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python build_kg.py --input ./docs
  python build_kg.py --input ./docs --fresh
  python build_kg.py --input ./docs --ext pdf,md
        """
    )
    parser.add_argument(
        "--input", "-i",
        type=Path,
        required=True,
        help="å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--ext", "-e",
        type=str,
        default="pdf,txt,md",
        help="å‡¦ç†ã™ã‚‹æ‹¡å¼µå­ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: pdf,txt,mdï¼‰"
    )
    parser.add_argument(
        "--fresh", "-f",
        action="store_true",
        help="æ–°è¦æ§‹ç¯‰ï¼ˆå‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼‰"
    )

    args = parser.parse_args()

    if not args.input.exists():
        print(f"âŒ ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.input}")
        sys.exit(1)

    extensions = [ext.strip().lower() for ext in args.ext.split(",")]

    print("ğŸš€ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰é–‹å§‹")
    print(f"   ãƒ¢ãƒ¼ãƒ‰: {'æ–°è¦æ§‹ç¯‰' if args.fresh else 'ç¶šãã‹ã‚‰å†é–‹'}")

    build_knowledge_graph(
        input_dir=args.input,
        extensions=extensions,
        fresh=args.fresh
    )

    print("\nâœ… å®Œäº†ï¼")
    print("   Streamlitã§ç¢ºèª: streamlit run app.py")


if __name__ == "__main__":
    main()
