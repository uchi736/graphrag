"""
Streamlit UI for Graph-RAG
===========================
ã‚·ãƒ³ãƒ—ãƒ«ãªGraph-RAGç”¨ã®Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- PDF/ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- è³ªå•å…¥åŠ›ã¨RAGå®Ÿè¡Œ
- ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®å¯è¦–åŒ–
"""
import os
import streamlit as st
from pathlib import Path
from dotenv import load_dotenv
import tempfile
from typing import List
import hashlib
import fitz  # PyMuPDF
import json

# LangChain imports
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
# LLM Factory for provider selection
from llm_factory import create_chat_llm, get_llm_provider_info
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_community.graphs import Neo4jGraph
from langchain_community.document_loaders import TextLoader
try:
    from langchain_community.graphs.graph_document import GraphDocument
except ImportError:
    from langchain_community.graphs import GraphDocument
from langchain_postgres import PGVector

# æ—¥æœ¬èªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢
from japanese_text_processor import get_japanese_processor, SUDACHI_AVAILABLE
from hybrid_retriever import HybridRetriever
from db_utils import normalize_pg_connection_string, ensure_tokenized_schema, ensure_hnsw_index

# ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–
from entity_vectorizer import EntityVectorizer

try:
    from langchain_community.retrievers.graph import GraphRetriever
except ImportError:
    try:
        from langchain_graph_retriever import GraphRetriever
    except ImportError:
        from langchain_graph_retriever.graph_retriever import GraphRetriever

try:
    from langchain_community.retrievers.parent_document import ParentDocumentRetriever
    HAS_PARENT = True
except ImportError:
    try:
        from langchain.retrievers.parent_document import ParentDocumentRetriever
        HAS_PARENT = True
    except ImportError:
        HAS_PARENT = False

from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnableParallel,
    RunnablePassthrough,
    RunnableLambda,
)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# Streamlitè¨­å®š
st.set_page_config(
    page_title="Graph-RAG Demo",
    page_icon="ğŸ”—",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ç®¡ç†ï¼ˆæ—©æœŸåˆæœŸåŒ–ï¼‰
if "graph_backend" not in st.session_state:
    st.session_state.graph_backend = os.getenv("GRAPH_BACKEND", "networkx").lower()

# ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«å¿œã˜ã¦å‹•çš„ã«å¤‰æ›´
st.title(f"ğŸ”— Graph-RAG with {st.session_state.graph_backend.upper()} & PGVector")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ç’°å¢ƒè¨­å®šç¢ºèª
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")

    AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
    AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
    AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION")
    AZURE_OPENAI_CHAT_DEPLOYMENT = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME")
    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME")

    NEO4J_URI = os.getenv("NEO4J_URI")
    NEO4J_USER = os.getenv("NEO4J_USER")
    NEO4J_PW = os.getenv("NEO4J_PW")
    PG_CONN = os.getenv("PG_CONN")
    if PG_CONN and not os.getenv("PGVECTOR_CONNECTION_STRING"):
        # Keep PGVector's expected env var in sync with the existing PG_CONN setting
        os.environ["PGVECTOR_CONNECTION_STRING"] = PG_CONN

    # å¿…é ˆç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆOpenAI, PGVectorï¼‰
    if not all([AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, PG_CONN]):
        st.error("ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # LLM Provider Status
    st.markdown("---")
    st.markdown("### ğŸ¤– LLM Provider")
    llm_info = get_llm_provider_info()
    st.info(f"{llm_info['status']}\n\nProvider: {llm_info['provider']}\nModel: {llm_info['model']}")

    st.markdown("---")
    st.markdown("### ğŸ—„ï¸ ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰")

    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠUI
    backend_options = {
        "NetworkX (è»½é‡ãƒ»Neo4jä¸è¦)": "networkx",
        "Neo4j (é«˜æ€§èƒ½ãƒ»å¤§è¦æ¨¡)": "neo4j"
    }

    current_backend_label = [k for k, v in backend_options.items()
                              if v == st.session_state.graph_backend][0]

    selected_backend = st.radio(
        "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é¸æŠ",
        list(backend_options.keys()),
        index=list(backend_options.values()).index(st.session_state.graph_backend),
        help="NetworkX: å³åº§ã«ä½¿ç”¨å¯èƒ½ã€å°ã€œä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ / Neo4j: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãƒ»é«˜åº¦ãªã‚¯ã‚¨ãƒª",
        label_visibility="collapsed"
    )

    # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰åˆ‡ã‚Šæ›¿ãˆæ¤œå‡º
    new_backend = backend_options[selected_backend]
    if new_backend != st.session_state.graph_backend:
        st.warning("âš ï¸ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã¨ã€æ—¢å­˜ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã¯ã‚¯ãƒªã‚¢ã•ã‚Œã¾ã™ã€‚")

        col1, col2 = st.columns(2)
        with col1:
            if st.button("âœ… åˆ‡ã‚Šæ›¿ãˆã‚‹", type="primary", use_container_width=True, key="switch_backend"):
                # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
                st.session_state.chain = None
                st.session_state.graph = None
                st.session_state.initialized = False
                st.session_state.uploaded_files = []
                st.session_state.existing_graph_loaded = False
                st.session_state.graph_data_cache = None
                st.session_state.graph_backend = new_backend
                st.success(f"âœ… {new_backend.upper()}ã«åˆ‡ã‚Šæ›¿ãˆã¾ã—ãŸ")
                st.rerun()
        with col2:
            if st.button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", use_container_width=True, key="cancel_switch"):
                st.rerun()
        st.stop()  # åˆ‡ã‚Šæ›¿ãˆç¢ºèªä¸­ã¯ä»¥é™ã®å‡¦ç†ã‚’åœæ­¢

    # Neo4jä½¿ç”¨æ™‚ã®ã¿Neo4jè¨­å®šã‚’å¿…é ˆåŒ–
    if st.session_state.graph_backend == "neo4j":
        if not all([NEO4J_URI, NEO4J_USER, NEO4J_PW]):
            st.error("âŒ Neo4jã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ NEO4J_URI, NEO4J_USER, NEO4J_PW ãŒå¿…è¦ã§ã™ã€‚")
            st.info("ğŸ’¡ NetworkXã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã¨ã€Neo4jè¨­å®šãªã—ã§ä½¿ç”¨ã§ãã¾ã™ã€‚")
            if st.button("NetworkXã«åˆ‡ã‚Šæ›¿ãˆ", key="fallback_to_networkx"):
                st.session_state.graph_backend = "networkx"
                st.rerun()
            st.stop()

        # Neo4jæ¥ç¶šãƒ†ã‚¹ãƒˆ
        try:
            with st.spinner("Neo4jæ¥ç¶šç¢ºèªä¸­..."):
                test_graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PW)
                del test_graph
            st.success(f"âœ… Neo4jæ¥ç¶šæˆåŠŸ")
        except Exception as e:
            st.error(f"âŒ Neo4jæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)[:100]}")
            st.info("ğŸ’¡ NetworkXã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã‹ï¼Ÿ")
            if st.button("NetworkXã«åˆ‡ã‚Šæ›¿ãˆ", key="fallback_on_error"):
                st.session_state.graph_backend = "networkx"
                st.rerun()
            st.stop()
    else:
        st.success(f"âœ… NetworkXãƒ¢ãƒ¼ãƒ‰ (Neo4jè¨­å®šä¸è¦)")

    st.markdown("---")
    st.markdown("### ğŸ“Š ã‚°ãƒ©ãƒ•å¯è¦–åŒ–è¨­å®š")

    viz_engine = st.radio(
        "å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³",
        ["Pyvis (æ¨å¥¨)", "Streamlit-Agraph"],
        index=0,
        help="Pyvisã¯é«˜åº¦ãªç‰©ç†æ¼”ç®—ã¨ãƒªãƒƒãƒãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã€Agraphã¯è»½é‡ã§ã‚·ãƒ³ãƒ—ãƒ«"
    )

    show_graph = st.checkbox("ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º", value=True)

    if show_graph:
        max_nodes = st.slider("æœ€å¤§è¡¨ç¤ºãƒãƒ¼ãƒ‰æ•°", 50, 500, 200, 50)

    st.markdown("---")
    st.markdown("### ğŸ” æ¤œç´¢è¨­å®š")

    # TopKè¨­å®šï¼ˆæ¤œç´¢çµæœæ•°ï¼‰
    retrieval_top_k = st.slider(
        "æ¤œç´¢çµæœæ•° (Top-K)",
        min_value=1,
        max_value=20,
        value=int(os.getenv("RETRIEVAL_TOP_K", "5")),
        step=1,
        help="RAGæ¤œç´¢ã§å–å¾—ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ã€‚å¤šã„ã»ã©æ–‡è„ˆãŒè±Šå¯Œã«ãªã‚Šã¾ã™ãŒã€å‡¦ç†æ™‚é–“ãŒå¢—åŠ ã—ã¾ã™ã€‚"
    )
    st.session_state.retrieval_top_k = retrieval_top_k

    # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ©Ÿèƒ½è¨­å®š
    st.markdown("---")
    st.markdown("### ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•")

    enable_knowledge_graph = st.checkbox(
        "ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚’æœ‰åŠ¹åŒ–",
        value=os.getenv("ENABLE_KNOWLEDGE_GRAPH", "true").lower() == "true",
        help="ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã¦ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ç”Ÿæˆã—ã¾ã™ã€‚å‡¦ç†æ™‚é–“ãŒå¢—åŠ ã—ã¾ã™ãŒã€ã‚ˆã‚Šé«˜åº¦ãªè³ªå•å¿œç­”ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"
    )
    st.session_state.enable_knowledge_graph = enable_knowledge_graph

    if enable_knowledge_graph:
        st.info("ğŸ” ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•: æœ‰åŠ¹\nã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã€ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ã®æ¨è«–ã‚’è¡Œã„ã¾ã™")

        # ã‚°ãƒ©ãƒ•æ¢ç´¢ãƒ›ãƒƒãƒ—æ•°è¨­å®š
        graph_hop_count = st.slider(
            "ã‚°ãƒ©ãƒ•æ¢ç´¢ãƒ›ãƒƒãƒ—æ•°",
            min_value=1,
            max_value=3,
            value=int(os.getenv("GRAPH_HOP_COUNT", "1")),
            step=1,
            help="1hop=ç›´æ¥é–¢ä¿‚ã®ã¿ã€2hop=å‹é”ã®å‹é”ã¾ã§ã€3hop=ã•ã‚‰ã«é–“æ¥çš„ãªé–¢ä¿‚ã¾ã§æ¢ç´¢"
        )
        st.session_state.graph_hop_count = graph_hop_count

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢è¨­å®š
        enable_entity_vector = st.checkbox(
            "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢",
            value=os.getenv("ENABLE_ENTITY_VECTOR_SEARCH", "true").lower() == "true",
            help="ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é¡ä¼¼åº¦æ¤œç´¢ã‚’æœ‰åŠ¹åŒ–ã€‚é¡ç¾©èªã‚„é–¢é€£èªã‚‚æ¤œç´¢å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"
        )
        st.session_state.enable_entity_vector = enable_entity_vector

        if enable_entity_vector:
            entity_similarity_threshold = st.slider(
                "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é¡ä¼¼åº¦é–¾å€¤",
                min_value=0.5,
                max_value=1.0,
                value=float(os.getenv("ENTITY_SIMILARITY_THRESHOLD", "0.7")),
                step=0.05,
                help="ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ã®é¡ä¼¼åº¦é–¾å€¤ã€‚ä½ã„ã»ã©å¹…åºƒãæ¤œç´¢ã—ã¾ã™ã€‚"
            )
            st.session_state.entity_similarity_threshold = entity_similarity_threshold
    else:
        st.warning("âš¡ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•: ç„¡åŠ¹\nãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ä½¿ç”¨ï¼ˆé«˜é€Ÿãƒ¢ãƒ¼ãƒ‰ï¼‰")

    # æ—¥æœ¬èªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢è¨­å®š
    if SUDACHI_AVAILABLE:
        enable_jp_search = st.checkbox(
            "æ—¥æœ¬èªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢",
            value=os.getenv("ENABLE_JAPANESE_SEARCH", "true").lower() == "true",
            help="ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ã¾ã™ï¼ˆç²¾åº¦å‘ä¸Šï¼‰"
        )

        if enable_jp_search:
            search_mode = st.radio(
                "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰",
                ["ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ¨å¥¨)", "ãƒ™ã‚¯ãƒˆãƒ«ã®ã¿", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿"],
                help="ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: RRFã§ã‚¹ã‚³ã‚¢çµ±åˆ / ãƒ™ã‚¯ãƒˆãƒ«: æ„å‘³æ¤œç´¢ / ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: å…¨æ–‡æ¤œç´¢"
            )

            # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
            mode_map = {
                "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ (æ¨å¥¨)": "hybrid",
                "ãƒ™ã‚¯ãƒˆãƒ«ã®ã¿": "vector",
                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿": "keyword"
            }
            st.session_state.search_mode = mode_map[search_mode]
            st.session_state.enable_japanese_search = True
        else:
            st.session_state.search_mode = "vector"
            st.session_state.enable_japanese_search = False
    else:
        st.warning("âš ï¸ sudachipyæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        st.caption("ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ä½¿ç”¨ã—ã¾ã™")
        with st.expander("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•"):
            st.code("pip install sudachipy sudachidict_core")
        st.session_state.search_mode = "vector"
        st.session_state.enable_japanese_search = False

    st.markdown("---")
    st.markdown("### ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†")

    if "confirm_delete" not in st.session_state:
        st.session_state.confirm_delete = False

    if not st.session_state.confirm_delete:
        if st.button("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢", use_container_width=True):
            st.session_state.confirm_delete = True
            st.rerun()
    else:
        st.warning("âš ï¸ æœ¬å½“ã«ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("âœ… ã¯ã„ã€å‰Šé™¤", type="primary", use_container_width=True):
                with st.spinner("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ä¸­..."):
                    try:
                        # ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¯ãƒªã‚¢
                        if st.session_state.graph_backend == "neo4j":
                            temp_graph = Neo4jGraph(
                                url=NEO4J_URI,
                                username=NEO4J_USER,
                                password=NEO4J_PW,
                                enhanced_schema=True
                            )
                            temp_graph.query("MATCH (n) DETACH DELETE n")
                        else:  # networkx
                            from networkx_graph import NetworkXGraph
                            temp_graph = NetworkXGraph(storage_path="graph.pkl", auto_save=True)
                            temp_graph.graph.clear()
                            temp_graph.node_metadata.clear()
                            temp_graph.edge_metadata.clear()
                            temp_graph.save()

                        # PGVectorã‚¯ãƒªã‚¢
                        from langchain_community.vectorstores import PGVector
                        try:
                            # PGVectorã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
                            import psycopg2
                            conn = psycopg2.connect(PG_CONN)
                            cur = conn.cursor()
                            cur.execute("DROP TABLE IF EXISTS langchain_pg_collection CASCADE")
                            cur.execute("DROP TABLE IF EXISTS langchain_pg_embedding CASCADE")
                            conn.commit()
                            cur.close()
                            conn.close()
                        except Exception as e:
                            st.warning(f"PGVectorã‚¯ãƒªã‚¢ã§è­¦å‘Š: {e}")

                        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆãƒªã‚»ãƒƒãƒˆ
                        st.session_state.chain = None
                        st.session_state.graph = None
                        st.session_state.initialized = False
                        st.session_state.uploaded_files = []
                        st.session_state.existing_graph_loaded = False
                        st.session_state.graph_data_cache = None
                        st.session_state.confirm_delete = False

                        st.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
                        st.rerun()
                    except Exception as e:
                        st.error(f"ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")
                        st.session_state.confirm_delete = False
        with col2:
            if st.button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«", use_container_width=True):
                st.session_state.confirm_delete = False
                st.rerun()

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if "chain" not in st.session_state:
    st.session_state.chain = None
if "graph" not in st.session_state:
    st.session_state.graph = None
if "initialized" not in st.session_state:
    st.session_state.initialized = False
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = []
if "existing_graph_loaded" not in st.session_state:
    st.session_state.existing_graph_loaded = False
if "graph_data_cache" not in st.session_state:
    st.session_state.graph_data_cache = None

# æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯é–¢æ•°ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å…±é€šï¼‰
def check_existing_graph(graph, backend: str) -> dict:
    """ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ—¢å­˜ã®ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    try:
        if backend == "neo4j":
            query = """
            MATCH (n)
            RETURN count(n) AS node_count
            """
            result = graph.query(query)
            node_count = result[0]['node_count'] if result else 0

            if node_count > 0:
                query_rel = """
                MATCH ()-[r]->()
                RETURN count(r) AS rel_count
                """
                result_rel = graph.query(query_rel)
                rel_count = result_rel[0]['rel_count'] if result_rel else 0

                return {
                    'exists': True,
                    'node_count': node_count,
                    'rel_count': rel_count
                }
        else:  # networkx
            node_count = graph.graph.number_of_nodes()
            rel_count = graph.graph.number_of_edges()

            if node_count > 0:
                return {
                    'exists': True,
                    'node_count': node_count,
                    'rel_count': rel_count
                }

        return {'exists': False, 'node_count': 0, 'rel_count': 0}
    except Exception as e:
        st.error(f"ã‚°ãƒ©ãƒ•æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return {'exists': False, 'node_count': 0, 'rel_count': 0}

# æ—¢å­˜ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ã‚’å¾©å…ƒ
def restore_from_existing_graph():
    """ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨PGVectorã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã‚·ã‚¹ãƒ†ãƒ ã‚’å¾©å…ƒ"""
    try:
        # ã‚°ãƒ©ãƒ•æ¥ç¶š
        if st.session_state.graph_backend == "neo4j":
            graph = Neo4jGraph(
                url=NEO4J_URI,
                username=NEO4J_USER,
                password=NEO4J_PW,
                enhanced_schema=False  # APOCä¸è¦
            )
        else:  # networkx
            from networkx_graph import NetworkXGraph
            graph = NetworkXGraph(storage_path="graph.pkl", auto_save=True)

        # PGVectoræ¥ç¶š
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
            openai_api_version=AZURE_OPENAI_API_VERSION,
            azure_endpoint=AZURE_OPENAI_ENDPOINT,
            api_key=AZURE_OPENAI_API_KEY
        )
        vector_store = PGVector(
            connection=PG_CONN,
            embeddings=embeddings
        )

        # Vector Retrieveræ§‹ç¯‰
        # TopKå€¤ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
        retrieval_top_k = st.session_state.get('retrieval_top_k', 5)

        if HAS_PARENT:
            vector_retriever = ParentDocumentRetriever(vector_store, search_kwargs={"k": retrieval_top_k})
        else:
            vector_retriever = vector_store.as_retriever(search_kwargs={"k": retrieval_top_k})

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºé–¢æ•°ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆï¼‰
        def extract_entities_from_question(question: str) -> List[str]:
            """LLMã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ã£ã¦è³ªå•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º"""
            entities = []

            # 1. LLMã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
            extraction_prompt = f"""ä»¥ä¸‹ã®è³ªå•æ–‡ã‹ã‚‰ã€å›ºæœ‰åè©ã‚„é‡è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆäººç‰©ã€å ´æ‰€ã€ç‰©ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

è³ªå•: {question}

ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£:"""
            try:
                llm = create_chat_llm(temperature=0)
                response = llm.invoke(extraction_prompt)
                llm_entities = [e.strip() for e in response.content.split(',') if e.strip()]
                entities.extend(llm_entities)
            except Exception:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
                entities.extend([w for w in question.split() if len(w) > 1])

            # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
            if st.session_state.get('enable_entity_vector', False):
                try:
                    entity_vectorizer = EntityVectorizer(PG_CONN, embeddings)

                    # è³ªå•ã®ãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¤œç´¢
                    similarity_threshold = st.session_state.get('entity_similarity_threshold', 0.7)
                    similar_entities = entity_vectorizer.search_similar_entities(
                        question,
                        k=10,
                        score_threshold=similarity_threshold
                    )

                    # æ¤œç´¢çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
                    if similar_entities:
                        print(f"[Entity Vector Search] Found {len(similar_entities)} similar entities")
                        for eid, score in similar_entities[:3]:
                            print(f"  - {eid}: {score:.3f}")

                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£IDã®ã¿ã‚’è¿½åŠ ï¼ˆé‡è¤‡æ’é™¤ï¼‰
                    for entity_id, score in similar_entities:
                        if entity_id not in entities:
                            entities.append(entity_id)

                except Exception as e:
                    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒå¤±æ•—ã—ã¦ã‚‚LLMçµæœã‚’ä½¿ç”¨
                    print(f"[Entity Vector Search Error] {e}")

            return entities

        def rank_relations_by_relevance(question: str, relations: list, top_k: int = 15) -> list:
            """LLMã‚’ä½¿ã£ã¦é–¢ä¿‚æ€§ã®è³ªå•ã¸ã®é–¢é€£åº¦ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
            if not relations:
                return []

            # é–¢ä¿‚æ€§ãƒªã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            relations_text = "\n".join([
                f"{i+1}. {r['start']} -[{r['type']}]-> {r['end']}"
                for i, r in enumerate(relations)
            ])

            ranking_prompt = f"""ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€å„ã‚°ãƒ©ãƒ•é–¢ä¿‚æ€§ã®é–¢é€£åº¦ã‚’0-10ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{question}

ã€ã‚°ãƒ©ãƒ•é–¢ä¿‚æ€§ã€‘
{relations_text}

ã€æŒ‡ç¤ºã€‘
- å„è¡Œã®ç•ªå·ã¨é–¢é€£åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-10ï¼‰ã‚’ã€Œç•ªå·:ã‚¹ã‚³ã‚¢ã€å½¢å¼ã§å‡ºåŠ›
- è³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹é–¢ä¿‚æ€§ã¯é«˜ã‚¹ã‚³ã‚¢ï¼ˆ8-10ï¼‰
- é–“æ¥çš„ã«é–¢é€£ã™ã‚‹é–¢ä¿‚æ€§ã¯ä¸­ã‚¹ã‚³ã‚¢ï¼ˆ4-7ï¼‰
- ç„¡é–¢ä¿‚ãªé–¢ä¿‚æ€§ã¯ä½ã‚¹ã‚³ã‚¢ï¼ˆ0-3ï¼‰
- èª¬æ˜ä¸è¦ã€ã‚¹ã‚³ã‚¢ã®ã¿å‡ºåŠ›

ã€å‡ºåŠ›ä¾‹ã€‘
1:9
2:3
3:7

ã€å‡ºåŠ›ã€‘"""

            try:
                llm = create_chat_llm(temperature=0)
                response = llm.invoke(ranking_prompt)

                # ã‚¹ã‚³ã‚¢ã‚’ãƒ‘ãƒ¼ã‚¹
                scores = {}
                for line in response.content.strip().split('\n'):
                    if ':' in line:
                        try:
                            idx, score = line.split(':')
                            scores[int(idx.strip())] = float(score.strip())
                        except:
                            continue

                # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½top_kä»¶ã‚’è¿”ã™
                ranked_relations = []
                for i, relation in enumerate(relations, 1):
                    score = scores.get(i, 0)
                    ranked_relations.append((score, relation))

                ranked_relations.sort(reverse=True, key=lambda x: x[0])
                return [rel for score, rel in ranked_relations[:top_k]]

            except Exception as e:
                # LLMãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¤±æ•—æ™‚ã¯å…ƒã®ãƒªã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
                return relations[:top_k]

        # ã‚°ãƒ©ãƒ•æ¤œç´¢é–¢æ•°ï¼ˆN-hopãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«å¯¾å¿œï¼‰
        def get_graph_context(question: str) -> list:
            """è³ªå•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡ºã—ã€N-hopãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«ã§ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’å–å¾—"""
            # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
            entities = extract_entities_from_question(question)
            if not entities:
                return []

            # 2. ãƒ›ãƒƒãƒ—æ•°ã‚’å–å¾—
            hop_count = st.session_state.get('graph_hop_count', 1)

            # 3. ãƒ›ãƒƒãƒ—æ•°ã«å¿œã˜ãŸã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
            if hop_count == 1:
                # 1-hop: ç›´æ¥é–¢ä¿‚ã®ã¿
                query = """
                UNWIND $entities AS entity
                MATCH (n)
                WHERE n.id CONTAINS entity
                AND NOT n.id =~ '[0-9a-f]{32}'
                WITH collect(DISTINCT n) AS matched_nodes

                UNWIND matched_nodes AS start_node
                MATCH (start_node)-[r]-(connected_node)
                WHERE type(r) <> 'MENTIONS'
                AND NOT connected_node.id =~ '[0-9a-f]{32}'

                WITH r, startNode(r) AS actual_start, endNode(r) AS actual_end
                RETURN DISTINCT actual_start.id AS start, type(r) AS type, actual_end.id AS end
                LIMIT 30
                """
                top_k = 15
            elif hop_count == 2:
                # 2-hop: å¯å¤‰é•·ãƒ‘ã‚¹ [*1..2]
                query = """
                UNWIND $entities AS entity
                MATCH (n)
                WHERE n.id CONTAINS entity
                AND NOT n.id =~ '[0-9a-f]{32}'
                WITH collect(DISTINCT n) AS matched_nodes

                UNWIND matched_nodes AS start_node
                MATCH path = (start_node)-[*1..2]-(end_node)
                WHERE ALL(r IN relationships(path) WHERE type(r) <> 'MENTIONS')
                AND ALL(node IN nodes(path) WHERE NOT node.id =~ '[0-9a-f]{32}')
                AND start_node <> end_node

                WITH relationships(path) AS rels
                UNWIND range(0, size(rels)-1) AS i
                WITH rels[i] AS r, startNode(rels[i]) AS s, endNode(rels[i]) AS e
                RETURN DISTINCT s.id AS start, type(r) AS type, e.id AS end
                LIMIT 50
                """
                top_k = 20
            else:  # hop_count == 3
                # 3-hop: å¯å¤‰é•·ãƒ‘ã‚¹ [*1..3]
                query = """
                UNWIND $entities AS entity
                MATCH (n)
                WHERE n.id CONTAINS entity
                AND NOT n.id =~ '[0-9a-f]{32}'
                WITH collect(DISTINCT n) AS matched_nodes

                UNWIND matched_nodes AS start_node
                MATCH path = (start_node)-[*1..3]-(end_node)
                WHERE ALL(r IN relationships(path) WHERE type(r) <> 'MENTIONS')
                AND ALL(node IN nodes(path) WHERE NOT node.id =~ '[0-9a-f]{32}')
                AND start_node <> end_node

                WITH relationships(path) AS rels
                UNWIND range(0, size(rels)-1) AS i
                WITH rels[i] AS r, startNode(rels[i]) AS s, endNode(rels[i]) AS e
                RETURN DISTINCT s.id AS start, type(r) AS type, e.id AS end
                LIMIT 80
                """
                top_k = 25

            try:
                result = graph.query(query, params={"entities": entities})
                if result:
                    # 4. LLMãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§é–¢é€£åº¦ã®é«˜ã„é–¢ä¿‚æ€§ã®ã¿ã«çµã‚‹
                    result = rank_relations_by_relevance(question, result, top_k=top_k)
                return result if result else []
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãª1-hopãƒãƒƒãƒãƒ³ã‚°
                fallback_query = """
                MATCH (n)-[r]->(m)
                WHERE (
                    ANY(entity IN $entities WHERE n.id CONTAINS entity OR m.id CONTAINS entity)
                )
                AND type(r) <> 'MENTIONS'
                AND NOT n.id =~ '[0-9a-f]{32}'
                AND NOT m.id =~ '[0-9a-f]{32}'
                RETURN DISTINCT n.id AS start, type(r) AS type, m.id AS end
                LIMIT 20
                """
                try:
                    result = graph.query(fallback_query, params={"entities": entities})
                    if result:
                        result = rank_relations_by_relevance(question, result, top_k=15)
                    return result if result else []
                except Exception:
                    return []

        # ãƒã‚§ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆGraph-First Retrievalï¼‰
        def retriever_and_merge(question: str):
            # 1. ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’å®Ÿè¡Œ
            triples = []
            enable_knowledge_graph = st.session_state.get('enable_knowledge_graph', True)

            if enable_knowledge_graph:
                triples = get_graph_context(question)

            # 2. ã‚°ãƒ©ãƒ•æ¤œç´¢çµæœãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è£œåŠ©çš„ã«ä½¿ç”¨
            docs = []
            if triples:
                # ã‚°ãƒ©ãƒ•ã‹ã‚‰é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—ã—ã€ãã‚Œã«é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                entity_names = list(set([t.get('start') for t in triples] + [t.get('end') for t in triples]))

                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
                if entity_names:
                    chunk_query = """
                    UNWIND $entity_names AS entity_name
                    MATCH (e {id: entity_name})<-[:MENTIONS]-(chunk)
                    WHERE chunk.id =~ '[0-9a-f]{32}'
                    RETURN DISTINCT chunk.id AS chunk_id, chunk.text AS text
                    LIMIT 5
                    """
                    try:
                        chunk_results = graph.query(chunk_query, params={"entity_names": entity_names})
                        if chunk_results:
                            # ã‚°ãƒ©ãƒ•ã‹ã‚‰å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ 
                            from langchain_core.documents import Document
                            docs = [Document(page_content=r.get('text', ''), metadata={'id': r.get('chunk_id')})
                                   for r in chunk_results if r.get('text')]
                    except Exception:
                        pass

            # 3. ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå–å¾—ã§ããªã„å ´åˆã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ç”¨
            if not docs:
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                if st.session_state.get('enable_japanese_search', False) and SUDACHI_AVAILABLE:
                    try:
                        hybrid_retriever = HybridRetriever(PG_CONN, collection_name="graphrag")
                        query_embedding = embeddings.embed_query(question)
                        search_type = st.session_state.get('search_mode', 'hybrid')

                        # TopKå€¤ã‚’å–å¾—
                        retrieval_top_k = st.session_state.get('retrieval_top_k', 5)

                        hybrid_results = hybrid_retriever.search(
                            query_text=question,
                            query_vector=query_embedding,
                            k=retrieval_top_k,
                            search_type=search_type
                        )

                        # LangChain Documentå½¢å¼ã«å¤‰æ›
                        from langchain_core.documents import Document
                        docs = [
                            Document(
                                page_content=r['text'],
                                metadata=r['metadata']
                            ) for r in hybrid_results
                        ]
                    except Exception as e:
                        st.warning(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
                        docs = vector_retriever.invoke(question)
                else:
                    # å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
                    docs = vector_retriever.invoke(question)

            graph_lines = [
                f"{t.get('start')} -[{t.get('type')}]â†’ {t.get('end')}"
                for t in triples
            ] if triples else ["(ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãªã—)"]

            context = (
                "<GRAPH_CONTEXT>\n" + "\n".join(graph_lines) + "\n</GRAPH_CONTEXT>\n\n" +
                "<DOCUMENT_CONTEXT>\n" + "\n---\n".join(d.page_content for d in docs) + "\n</DOCUMENT_CONTEXT>"
            )
            return {
                "context": context,
                "question": question,
                "vector_sources": docs,
                "graph_sources": triples
            }

        prompt = PromptTemplate.from_template(
            """ã‚ãªãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚\nè³ªå•: {question}\n\n{context}\n\n---\nä¸Šè¨˜æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã«ã€æ—¥æœ¬èªã§ç¶²ç¾…çš„ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"""
        )

        # LLMå‘¼ã³å‡ºã—éƒ¨åˆ†
        llm_chain = (
            prompt
            | create_chat_llm(temperature=0)
            | StrOutputParser()
        )

        # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿æŒã™ã‚‹é–¢æ•°
        def generate_with_sources(data):
            answer = llm_chain.invoke({"question": data["question"], "context": data["context"]})
            return {
                "answer": answer,
                "vector_sources": data["vector_sources"],
                "graph_sources": data["graph_sources"]
            }

        chain = (
            RunnablePassthrough()
            | RunnableLambda(retriever_and_merge)
            | RunnableLambda(generate_with_sources)
        )

        return chain, graph

    except Exception as e:
        raise Exception(f"ã‚·ã‚¹ãƒ†ãƒ å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿é–¢æ•°
def load_documents(uploaded_files) -> list:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆã‚½ãƒ¼ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰"""
    from langchain_core.documents import Document
    all_docs = []

    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name

        try:
            file_name = uploaded_file.name
            if uploaded_file.name.endswith('.pdf'):
                # PyMuPDF (fitz) ã§é«˜ç²¾åº¦æŠ½å‡º
                pdf_doc = fitz.open(tmp_path)
                text_parts = []
                for page_num in range(len(pdf_doc)):
                    page = pdf_doc[page_num]
                    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆä¿æŒãƒ»ã‚½ãƒ¼ãƒˆä»˜ãã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                    text = page.get_text("text", sort=True)
                    if text.strip():  # ç©ºãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
                        text_parts.append(text)
                pdf_doc.close()
                text_content = "\n\n".join(text_parts)
            elif uploaded_file.name.endswith('.txt'):
                loader = TextLoader(tmp_path, encoding='utf-8')
                docs = loader.load()
                text_content = "\n".join([doc.page_content for doc in docs])
            else:
                # ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
                text_content = uploaded_file.getvalue().decode('utf-8')

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
            all_docs.append(Document(
                page_content=text_content,
                metadata={"source": file_name}
            ))
        finally:
            os.unlink(tmp_path)

    return all_docs

# åˆæœŸåŒ–é–¢æ•°
def load_csv_edges(uploaded_file):
    """CSV( source,target,label ) ã‚’èª­ã¿è¾¼ã¿ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    if not uploaded_file:
        return []
    import csv
    import io

    # UTF-8-sig (BOMä»˜ã) ã«ã‚‚å¯¾å¿œ
    try:
        text = uploaded_file.getvalue().decode("utf-8-sig")
    except Exception:
        try:
            text = uploaded_file.getvalue().decode("utf-8")
        except Exception:
            text = uploaded_file.getvalue().decode("utf-8", errors="ignore")

    reader = csv.DictReader(io.StringIO(text))
    edges = []

    for row in reader:
        if not row:
            continue

        # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç©ºç™½ã‚‚è€ƒæ…®ã—ã¦ã‚­ãƒ¼ã‚’æ­£è¦åŒ–ï¼ˆå°æ–‡å­—åŒ–ãƒ»ç©ºç™½é™¤å»ï¼‰
        normalized_row = {k.strip().lower() if k else k: v for k, v in row.items()}

        src = (normalized_row.get("source") or normalized_row.get("from") or normalized_row.get("src") or "").strip()
        tgt = (normalized_row.get("target") or normalized_row.get("to") or normalized_row.get("dst") or "").strip()
        rel = (normalized_row.get("label") or normalized_row.get("relation") or normalized_row.get("rel") or "RELATED_TO").strip()

        if not src or not tgt:
            continue
        edges.append({"source": src, "target": tgt, "label": rel})

    return edges


def build_rag_system(source_docs: list, csv_edges: list | None = None):
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰"""

    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆRecursiveCharacterTextSplitter: é‡è¤‡ã‚’é˜²ãï¼‰
    embeddings = AzureOpenAIEmbeddings(
        azure_deployment=AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
        openai_api_version=AZURE_OPENAI_API_VERSION,
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_API_KEY
    )
    chunker = RecursiveCharacterTextSplitter(
        chunk_size=500,           # 500æ–‡å­—ã”ã¨ã«åˆ†å‰²
        chunk_overlap=100,        # 100æ–‡å­—ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡è„ˆä¿æŒï¼‰
        separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""],  # æ—¥æœ¬èªå¯¾å¿œ
        length_function=len
    )

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
    all_chunks = []
    for doc in source_docs:
        doc_chunks = chunker.create_documents([doc.page_content])
        # å„ãƒãƒ£ãƒ³ã‚¯ã«ã‚½ãƒ¼ã‚¹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸
        for chunk in doc_chunks:
            chunk.metadata.update(doc.metadata)
        all_chunks.extend(doc_chunks)

    chunks = all_chunks

    # ãƒãƒ£ãƒ³ã‚¯é‡è¤‡é™¤å»ï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
    deduped = []
    seen_hashes = set()
    for chunk in chunks:
        digest = hashlib.sha256(chunk.page_content.encode("utf-8")).hexdigest()
        if digest in seen_hashes:
            continue
        seen_hashes.add(digest)
        chunk.metadata["id"] = digest
        deduped.append(chunk)
    chunks = deduped

    # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ©Ÿèƒ½ã®ãƒã‚§ãƒƒã‚¯
    enable_knowledge_graph = st.session_state.get('enable_knowledge_graph', True)

    if enable_knowledge_graph:
        st.info("ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")

        # GraphDocumentåŒ–
        llm = create_chat_llm(temperature=0)

        # ã‚«ã‚¹ã‚¿ãƒ KGæŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå°‚é–€ç”¨èªï¼‹åŒ…æ‹¬çš„ãªé–¢ä¿‚ã‚¿ã‚¤ãƒ—ï¼‰
        kg_system_prompt = """
ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å°‚é–€ç”¨èªã¨ãã®é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å°‚é–€ç”¨èªãƒãƒ¼ãƒ‰ã¨é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€ãƒãƒ¼ãƒ‰æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
- å°‚é–€ç”¨èªï¼ˆTermï¼‰ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
- å°‚é–€ç”¨èªã®ä¾‹:
  - æŠ€è¡“ç”¨èª: APIã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€æ©Ÿæ¢°å­¦ç¿’
  - åŒ»ç™‚ç”¨èª: ç–¾æ‚£åã€è–¬å‰¤åã€æ²»ç™‚æ³•
  - æ³•å¾‹ç”¨èª: æ³•ä»¤åã€å¥‘ç´„æ¡é …ã€æ³•çš„æ¦‚å¿µ
  - ãƒ“ã‚¸ãƒã‚¹ç”¨èª: KPIã€ROIã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³
  - å­¦è¡“ç”¨èª: ç†è«–åã€æ–¹æ³•è«–ã€æ¦‚å¿µ
  - ãƒ—ãƒ­ã‚»ã‚¹ãƒ»æ‰‹é †: å·¥ç¨‹åã€ã‚¹ãƒ†ãƒƒãƒ—ã€ãƒ•ã‚§ãƒ¼ã‚º
- ä¸€èˆ¬çš„ãªåè©ã‚„å‹•è©ã¯ç„¡è¦–ã—ã¦ãã ã•ã„ï¼ˆã€Œäººã€ã€Œç‰©ã€ã€Œã™ã‚‹ã€ã€Œè¡Œã†ã€ãªã©ï¼‰
- å›ºæœ‰åè©ã¯å°‚é–€ç”¨èªã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„

ã€è¡¨è¨˜ã‚†ã‚Œã®çµ±ä¸€ã€‘
- åŒã˜æ¦‚å¿µã‚’æŒ‡ã™ç•°ãªã‚‹è¡¨è¨˜ã¯åŒä¸€ãƒãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
  ä¾‹: ã€ŒAIã€ã€Œäººå·¥çŸ¥èƒ½ã€ã€ŒArtificial Intelligenceã€â†’ã€ŒAIã€
  ä¾‹: ã€ŒDBã€ã€Œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€â†’ã€Œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€
  ä¾‹: ã€ŒMLã€ã€Œæ©Ÿæ¢°å­¦ç¿’ã€ã€ŒMachine Learningã€â†’ã€Œæ©Ÿæ¢°å­¦ç¿’ã€

ã€ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
ä»¥ä¸‹ã®ã‚«ãƒ†ã‚´ãƒªã®é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

**1. éšå±¤ãƒ»åˆ†é¡é–¢ä¿‚**
- IS_A: ä¸Šä½ä¸‹ä½é–¢ä¿‚ï¼ˆå…·ä½“â†’æŠ½è±¡ï¼‰
  ä¾‹: ã€ŒMySQLã€-[IS_A]->ã€Œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€
- BELONGS_TO_CATEGORY: ã‚«ãƒ†ã‚´ãƒªæ‰€å±
  ä¾‹: ã€Œæ±ºç®—æ›¸ã€-[BELONGS_TO_CATEGORY]->ã€Œè²¡å‹™æ›¸é¡ã€
- PART_OF: éƒ¨åˆ†æ§‹æˆé–¢ä¿‚
  ä¾‹: ã€Œã‚¨ãƒ³ã‚¸ãƒ³ã€-[PART_OF]->ã€Œè‡ªå‹•è»Šã€
- HAS_STEP: ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¹ãƒ†ãƒƒãƒ—
  ä¾‹: ã€Œè¦ä»¶å®šç¾©ã€-[HAS_STEP]->ã€Œã‚·ã‚¹ãƒ†ãƒ é–‹ç™ºã€

**2. å±æ€§ãƒ»ç‰¹æ€§é–¢ä¿‚**
- HAS_ATTRIBUTE: å±æ€§ä¿æŒ
  ä¾‹: ã€Œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€-[HAS_ATTRIBUTE]->ã€ŒACIDç‰¹æ€§ã€
- RELATED_TO: ä¸€èˆ¬çš„ãªé–¢é€£æ€§
  ä¾‹: ã€Œã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€-[RELATED_TO]->ã€Œèªè¨¼ã€

**3. å› æœãƒ»ä¾å­˜é–¢ä¿‚**
- AFFECTS: å½±éŸ¿é–¢ä¿‚
  ä¾‹: ã€Œé‡‘åˆ©ã€-[AFFECTS]->ã€Œä½å®…ãƒ­ãƒ¼ãƒ³ã€
- CAUSES: åŸå› çµæœ
  ä¾‹: ã€Œãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã€-[CAUSES]->ã€Œã‚·ã‚¹ãƒ†ãƒ ãƒ€ã‚¦ãƒ³ã€
- DEPENDS_ON: ä¾å­˜é–¢ä¿‚
  ä¾‹: ã€Œãƒ‡ãƒ—ãƒ­ã‚¤ã€-[DEPENDS_ON]->ã€Œãƒ†ã‚¹ãƒˆå®Œäº†ã€

**4. é©ç”¨ãƒ»åˆ¶ç´„é–¢ä¿‚**
- APPLIES_TO: é©ç”¨å¯¾è±¡
  ä¾‹: ã€ŒGDPRã€-[APPLIES_TO]->ã€Œå€‹äººæƒ…å ±ã€
- APPLIES_WHEN: é©ç”¨æ¡ä»¶
  ä¾‹: ã€Œç·Šæ€¥å¯¾å¿œæ‰‹é †ã€-[APPLIES_WHEN]->ã€Œéšœå®³ç™ºç”Ÿæ™‚ã€
- REQUIRES_QUALITY_GATE: å“è³ªã‚²ãƒ¼ãƒˆè¦æ±‚
  ä¾‹: ã€Œæœ¬ç•ªãƒªãƒªãƒ¼ã‚¹ã€-[REQUIRES_QUALITY_GATE]->ã€Œã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã€
- REQUIRES_APPROVAL_FROM: æ‰¿èªè¦æ±‚
  ä¾‹: ã€Œäºˆç®—åŸ·è¡Œã€-[REQUIRES_APPROVAL_FROM]->ã€Œå–ç· å½¹ä¼šã€

**5. æ‰€æœ‰ãƒ»è²¬ä»»é–¢ä¿‚**
- OWNED_BY: æ‰€æœ‰è€…
  ä¾‹: ã€Œèªè¨¼ã‚µãƒ¼ãƒ“ã‚¹ã€-[OWNED_BY]->ã€Œã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒ¼ãƒ ã€

**6. åŒç¾©èªé–¢ä¿‚**
- SAME_AS: å®Œå…¨åŒç¾©
  ä¾‹: ã€ŒAIã€-[SAME_AS]->ã€Œäººå·¥çŸ¥èƒ½ã€
- ALIAS_OF: ã‚¨ã‚¤ãƒªã‚¢ã‚¹ãƒ»ç•¥ç§°
  ä¾‹: ã€ŒDBã€-[ALIAS_OF]->ã€Œãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€

ã€é‡è¦ãªæ³¨æ„äº‹é …ã€‘
- æ˜ç¢ºãªé–¢ä¿‚æ€§ã®ã¿ã‚’æŠ½å‡ºã—ã€æ¨æ¸¬ã‚„æ›–æ˜§ãªé–¢ä¿‚ã¯å«ã‚ãªã„ã§ãã ã•ã„
- é–¢ä¿‚ã®æ–¹å‘æ€§ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼ˆç‰¹ã«IS_Aã€PART_OFãªã©ï¼‰
- ãƒ†ã‚­ã‚¹ãƒˆä¸­ã«æ˜ç¤ºã•ã‚Œã¦ã„ã‚‹é–¢ä¿‚ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
"""

        kg_user_prompt = """
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ä¸Šè¨˜ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å°‚é–€ç”¨èªã¨ãã®é–¢ä¿‚æ€§ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{input}
"""

        kg_prompt = ChatPromptTemplate.from_messages([
            ("system", kg_system_prompt),
            ("user", kg_user_prompt)
        ])

        transformer = LLMGraphTransformer(
            llm=llm,
            prompt=kg_prompt,
            allowed_nodes=["Term"],
            allowed_relationships=[
                # éšå±¤ãƒ»åˆ†é¡é–¢ä¿‚
                "IS_A", "BELONGS_TO_CATEGORY", "PART_OF", "HAS_STEP",
                # å±æ€§ãƒ»ç‰¹æ€§é–¢ä¿‚
                "HAS_ATTRIBUTE", "RELATED_TO",
                # å› æœãƒ»ä¾å­˜é–¢ä¿‚
                "AFFECTS", "CAUSES", "DEPENDS_ON",
                # é©ç”¨ãƒ»åˆ¶ç´„é–¢ä¿‚
                "APPLIES_TO", "APPLIES_WHEN", "REQUIRES_QUALITY_GATE", "REQUIRES_APPROVAL_FROM",
                # æ‰€æœ‰ãƒ»è²¬ä»»é–¢ä¿‚
                "OWNED_BY",
                # åŒç¾©èªé–¢ä¿‚
                "SAME_AS", "ALIAS_OF"
            ],
            strict_mode=True
        )
        graph_docs = transformer.convert_to_graph_documents(chunks)

        # ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ãƒ­ãƒ¼ãƒ‰
        if st.session_state.graph_backend == "neo4j":
            graph = Neo4jGraph(
                url=NEO4J_URI,
                username=NEO4J_USER,
                password=NEO4J_PW,
                enhanced_schema=True  # APOCã‚’ä½¿ç”¨
            )
            graph.add_graph_documents(graph_docs, include_source=True)
        else:  # networkx
            from networkx_graph import NetworkXGraph
            graph = NetworkXGraph(storage_path="graph.pkl", auto_save=True)
            graph.add_graph_documents(graph_docs, include_source=True)

        # CSVã‚¨ãƒƒã‚¸å–ã‚Šè¾¼ã¿ï¼ˆsource,target,label ã®ã‚·ãƒ³ãƒ—ãƒ«å½¢å¼ï¼‰
        if csv_edges:
            if st.session_state.graph_backend == "neo4j":
                for edge in csv_edges:
                    graph.query(
                        f"""
                        MERGE (s:CSVNode {{id: $src}})
                        MERGE (t:CSVNode {{id: $tgt}})
                        MERGE (s)-[r:`{edge['label']}`]->(t)
                        """,
                        params={"src": edge["source"], "tgt": edge["target"]}
                    )
            else:
                # NetworkXGraphã¯Cypher MERGEã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ã®ã§æ‰‹å‹•è¿½åŠ 
                for edge in csv_edges:
                    src = edge["source"]
                    tgt = edge["target"]
                    rel = edge["label"]
                    graph.add_node_manual(src, node_type="CSVNode")
                    graph.add_node_manual(tgt, node_type="CSVNode")
                    graph.add_edge_manual(src, tgt, rel_type=rel)
                if getattr(graph, "auto_save", False):
                    graph.save()

        # Documentãƒãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦Chunkã¨ãƒªãƒ³ã‚¯
        for doc in source_docs:
            doc_name = doc.metadata.get("source", "Unknown")
            # Documentãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
            graph.query("""
                MERGE (d:Document {name: $doc_name})
                SET d.created = timestamp()
            """, params={"doc_name": doc_name})

        # å„Chunkã‚’Documentã«ãƒªãƒ³ã‚¯
        for chunk in chunks:
            chunk_id = chunk.metadata.get("id")
            doc_name = chunk.metadata.get("source", "Unknown")
            if chunk_id:
                graph.query("""
                    MATCH (c:Chunk {id: $chunk_id})
                    MATCH (d:Document {name: $doc_name})
                    MERGE (c)-[:FROM_DOCUMENT]->(d)
                """, params={"chunk_id": chunk_id, "doc_name": doc_name})

        # ã‚¯ãƒ­ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¨è«–: å…±é€šã™ã‚‹å°‚é–€ç”¨èªã‚’æŒã¤ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé–“ã«ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        cross_doc_query = """
        MATCH (d1:Document)<-[:FROM_DOCUMENT]-(c1:Chunk)-[:MENTIONS]->(term:Term)
        MATCH (d2:Document)<-[:FROM_DOCUMENT]-(c2:Chunk)-[:MENTIONS]->(term)
        WHERE d1.name <> d2.name
        WITH d1, d2, COUNT(DISTINCT term) AS common_terms
        WHERE common_terms >= 2
        MERGE (d1)-[r:SHARES_TOPICS_WITH]->(d2)
        SET r.common_term_count = common_terms
        """
        try:
            graph.query(cross_doc_query)
        except Exception as e:
            # ã‚¯ãƒ­ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¨è«–ãŒå¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
            pass

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if st.session_state.get('enable_entity_vector', True):
            with st.spinner("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­..."):
                try:
                    entity_vectorizer = EntityVectorizer(PG_CONN, embeddings)

                    # ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
                    entities = entity_vectorizer.extract_entities_from_graph(
                        graph,
                        graph_backend=st.session_state.graph_backend
                    )

                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜
                    num_saved = entity_vectorizer.add_entities(entities, graph_docs)

                    if num_saved > 0:
                        st.success(f"âœ… {num_saved}å€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã—ãŸ")

                except Exception as e:
                    st.warning(f"ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        st.info("âš¡ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™")
        # LLMã¯ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ã§å¿…è¦
        llm = create_chat_llm(temperature=0)
        graph_docs = []  # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•OFFã®å ´åˆã¯ç©º

        # ã‚°ãƒ©ãƒ•ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ä½œæˆï¼ˆCSVã‚¨ãƒƒã‚¸ç”¨ï¼‰
        if st.session_state.graph_backend == "neo4j":
            graph = Neo4jGraph(
                url=NEO4J_URI,
                username=NEO4J_USER,
                password=NEO4J_PW,
                enhanced_schema=True
            )
        else:  # networkx
            from networkx_graph import NetworkXGraph
            graph = NetworkXGraph(storage_path="graph.pkl", auto_save=True)

        # CSVã‚¨ãƒƒã‚¸å–ã‚Šè¾¼ã¿ï¼ˆãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•OFFã§ã‚‚CSVã¯å‡¦ç†ï¼‰
        if csv_edges:
            st.info(f"ğŸ”— CSVã‹ã‚‰{len(csv_edges)}ä»¶ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ä¸­...")
            if st.session_state.graph_backend == "neo4j":
                for edge in csv_edges:
                    graph.query(
                        f"""
                        MERGE (s:CSVNode {{id: $src}})
                        MERGE (t:CSVNode {{id: $tgt}})
                        MERGE (s)-[r:`{edge['label']}`]->(t)
                        """,
                        params={"src": edge["source"], "tgt": edge["target"]}
                    )
            else:
                for edge in csv_edges:
                    src = edge["source"]
                    tgt = edge["target"]
                    rel = edge["label"]
                    graph.add_node_manual(src, node_type="CSVNode")
                    graph.add_node_manual(tgt, node_type="CSVNode")
                    graph.add_edge_manual(src, tgt, rel_type=rel)
                if getattr(graph, "auto_save", False):
                    graph.save()
            st.success(f"âœ… CSVã‹ã‚‰{len(csv_edges)}ä»¶ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

        # CSVã‚¨ãƒƒã‚¸ã‹ã‚‰ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if csv_edges and st.session_state.get('enable_entity_vector', True):
            with st.spinner("CSVã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ä¸­..."):
                try:
                    entity_vectorizer = EntityVectorizer(PG_CONN, embeddings)

                    # ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
                    entities = entity_vectorizer.extract_entities_from_graph(
                        graph,
                        graph_backend=st.session_state.graph_backend
                    )

                    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ä¿å­˜
                    num_saved = entity_vectorizer.add_entities(entities, [])

                    if num_saved > 0:
                        st.success(f"âœ… {num_saved}å€‹ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã—ãŸ")

                except Exception as e:
                    st.warning(f"ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    # æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
    japanese_processor = get_japanese_processor()
    if japanese_processor and st.session_state.get('enable_japanese_search', True):
        with st.spinner("æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ä¸­..."):
            for chunk in chunks:
                try:
                    tokenized = japanese_processor.tokenize(chunk.page_content)
                    chunk.metadata['tokenized_content'] = tokenized
                except Exception as e:
                    st.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}")
                    chunk.metadata['tokenized_content'] = None

    # PGVectorä¿å­˜ï¼ˆé‡è¤‡é˜²æ­¢è¨­å®šä»˜ãï¼‰
    # ãƒãƒ£ãƒ³ã‚¯ãŒ0ä»¶ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆCSVã®ã¿ã®å ´åˆãªã©ï¼‰
    if not chunks:
        st.warning("ãƒãƒ£ãƒ³ã‚¯ãŒ0ä»¶ã®ãŸã‚ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
        vector_store = None
    else:
        # IDã®NULLãƒã‚§ãƒƒã‚¯
        ids = []
        for c in chunks:
            cid = c.metadata.get("id")
            if not cid:
                raise ValueError("Chunk metadata ã« id ãŒã‚ã‚Šã¾ã›ã‚“")
            ids.append(cid)

        ensure_hnsw_index(PG_CONN)
        vector_store = PGVector.from_documents(
            chunks,
            embeddings,
            connection=PG_CONN,
            collection_name="graphrag",
            pre_delete_collection=True,  # æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤
            ids=ids,  # IDæŒ‡å®šã§é‡è¤‡é˜²æ­¢
            use_jsonb=True,
        )

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’DBã«åæ˜ 
    if vector_store and japanese_processor and st.session_state.get('enable_japanese_search', True):
        try:
            ensure_tokenized_schema(PG_CONN)
            import psycopg
            raw_pg_conn = normalize_pg_connection_string(PG_CONN)
            with psycopg.connect(raw_pg_conn) as conn:
                with conn.cursor() as cur:
                    for chunk in chunks:
                        tokenized = chunk.metadata.get('tokenized_content')
                        if tokenized:
                            cur.execute("""
                                UPDATE langchain_pg_embedding
                                SET tokenized_content = %s
                                WHERE cmetadata->>'id' = %s
                            """, (tokenized, chunk.metadata['id']))
                conn.commit()
        except Exception as e:
            st.warning(f"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‡ãƒ¼ã‚¿ã®DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # Vector Retrieveræ§‹ç¯‰
    # TopKå€¤ã‚’å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰
    retrieval_top_k = st.session_state.get('retrieval_top_k', 5)

    # vector_storeãŒNoneï¼ˆCSVã®ã¿ï¼‰ã®å ´åˆã¯retrieverã‚‚None
    if vector_store is None:
        vector_retriever = None
    elif HAS_PARENT:
        vector_retriever = ParentDocumentRetriever(vector_store, search_kwargs={"k": retrieval_top_k})
    else:
        vector_retriever = vector_store.as_retriever(search_kwargs={"k": retrieval_top_k})

    # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºé–¢æ•°ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆï¼‰
    def extract_entities_from_question(question: str) -> List[str]:
        """LLMã¨ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ã£ã¦è³ªå•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º"""
        entities = []

        # 1. LLMã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        extraction_prompt = f"""ä»¥ä¸‹ã®è³ªå•æ–‡ã‹ã‚‰ã€å›ºæœ‰åè©ã‚„é‡è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆäººç‰©ã€å ´æ‰€ã€ç‰©ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚

è³ªå•: {question}

ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£:"""
        try:
            response = llm.invoke(extraction_prompt)
            llm_entities = [e.strip() for e in response.content.split(',') if e.strip()]
            entities.extend(llm_entities)
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            entities.extend([w for w in question.split() if len(w) > 1])

        # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
        if st.session_state.get('enable_entity_vector', False):
            try:
                entity_vectorizer = EntityVectorizer(PG_CONN, embeddings)

                # è³ªå•ã®ãƒ™ã‚¯ãƒˆãƒ«ã§é¡ä¼¼ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¤œç´¢
                similarity_threshold = st.session_state.get('entity_similarity_threshold', 0.7)
                similar_entities = entity_vectorizer.search_similar_entities(
                    question,
                    k=10,
                    score_threshold=similarity_threshold
                )

                # æ¤œç´¢çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
                if similar_entities:
                    print(f"[Entity Vector Search] Found {len(similar_entities)} similar entities")
                    for eid, score in similar_entities[:3]:
                        print(f"  - {eid}: {score:.3f}")

                # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£IDã®ã¿ã‚’è¿½åŠ ï¼ˆé‡è¤‡æ’é™¤ï¼‰
                for entity_id, score in similar_entities:
                    if entity_id not in entities:
                        entities.append(entity_id)

            except Exception as e:
                # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒå¤±æ•—ã—ã¦ã‚‚LLMçµæœã‚’ä½¿ç”¨
                print(f"[Entity Vector Search Error] {e}")

        return entities

    def rank_relations_by_relevance(question: str, relations: list, top_k: int = 15) -> list:
        """LLMã‚’ä½¿ã£ã¦é–¢ä¿‚æ€§ã®è³ªå•ã¸ã®é–¢é€£åº¦ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        if not relations:
            return []

        # é–¢ä¿‚æ€§ãƒªã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
        relations_text = "\n".join([
            f"{i+1}. {r['start']} -[{r['type']}]-> {r['end']}"
            for i, r in enumerate(relations)
        ])

        ranking_prompt = f"""ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€å„ã‚°ãƒ©ãƒ•é–¢ä¿‚æ€§ã®é–¢é€£åº¦ã‚’0-10ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚

ã€è³ªå•ã€‘
{question}

ã€ã‚°ãƒ©ãƒ•é–¢ä¿‚æ€§ã€‘
{relations_text}

ã€æŒ‡ç¤ºã€‘
- å„è¡Œã®ç•ªå·ã¨é–¢é€£åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-10ï¼‰ã‚’ã€Œç•ªå·:ã‚¹ã‚³ã‚¢ã€å½¢å¼ã§å‡ºåŠ›
- è³ªå•ã«ç›´æ¥é–¢é€£ã™ã‚‹é–¢ä¿‚æ€§ã¯é«˜ã‚¹ã‚³ã‚¢ï¼ˆ8-10ï¼‰
- é–“æ¥çš„ã«é–¢é€£ã™ã‚‹é–¢ä¿‚æ€§ã¯ä¸­ã‚¹ã‚³ã‚¢ï¼ˆ4-7ï¼‰
- ç„¡é–¢ä¿‚ãªé–¢ä¿‚æ€§ã¯ä½ã‚¹ã‚³ã‚¢ï¼ˆ0-3ï¼‰
- èª¬æ˜ä¸è¦ã€ã‚¹ã‚³ã‚¢ã®ã¿å‡ºåŠ›

ã€å‡ºåŠ›ä¾‹ã€‘
1:9
2:3
3:7

ã€å‡ºåŠ›ã€‘"""

        try:
            response = llm.invoke(ranking_prompt)

            # ã‚¹ã‚³ã‚¢ã‚’ãƒ‘ãƒ¼ã‚¹
            scores = {}
            for line in response.content.strip().split('\n'):
                if ':' in line:
                    try:
                        idx, score = line.split(':')
                        scores[int(idx.strip())] = float(score.strip())
                    except:
                        continue

            # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½top_kä»¶ã‚’è¿”ã™
            ranked_relations = []
            for i, relation in enumerate(relations, 1):
                score = scores.get(i, 0)
                ranked_relations.append((score, relation))

            ranked_relations.sort(reverse=True, key=lambda x: x[0])
            return [rel for score, rel in ranked_relations[:top_k]]

        except Exception as e:
            # LLMãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°å¤±æ•—æ™‚ã¯å…ƒã®ãƒªã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
            return relations[:top_k]

    # ã‚°ãƒ©ãƒ•æ¤œç´¢é–¢æ•°ï¼ˆN-hopãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«å¯¾å¿œï¼‰
    def get_graph_context(question: str) -> list:
        """è³ªå•ã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡ºã—ã€N-hopãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«ã§ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’å–å¾—"""
        # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
        entities = extract_entities_from_question(question)
        if not entities:
            return []

        # 2. ãƒ›ãƒƒãƒ—æ•°ã‚’å–å¾—
        hop_count = st.session_state.get('graph_hop_count', 1)

        # 3. ãƒ›ãƒƒãƒ—æ•°ã«å¿œã˜ãŸã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        if hop_count == 1:
            # 1-hop: ç›´æ¥é–¢ä¿‚ã®ã¿
            query = """
            UNWIND $entities AS entity
            MATCH (n)
            WHERE n.id CONTAINS entity
            AND NOT n.id =~ '[0-9a-f]{32}'
            WITH collect(DISTINCT n) AS matched_nodes

            UNWIND matched_nodes AS start_node
            MATCH (start_node)-[r]-(connected_node)
            WHERE type(r) <> 'MENTIONS'
            AND NOT connected_node.id =~ '[0-9a-f]{32}'

            WITH r, startNode(r) AS actual_start, endNode(r) AS actual_end
            RETURN DISTINCT actual_start.id AS start, type(r) AS type, actual_end.id AS end
            LIMIT 30
            """
            top_k = 15
        elif hop_count == 2:
            # 2-hop: å¯å¤‰é•·ãƒ‘ã‚¹ [*1..2]
            query = """
            UNWIND $entities AS entity
            MATCH (n)
            WHERE n.id CONTAINS entity
            AND NOT n.id =~ '[0-9a-f]{32}'
            WITH collect(DISTINCT n) AS matched_nodes

            UNWIND matched_nodes AS start_node
            MATCH path = (start_node)-[*1..2]-(end_node)
            WHERE ALL(r IN relationships(path) WHERE type(r) <> 'MENTIONS')
            AND ALL(node IN nodes(path) WHERE NOT node.id =~ '[0-9a-f]{32}')
            AND start_node <> end_node

            WITH relationships(path) AS rels
            UNWIND range(0, size(rels)-1) AS i
            WITH rels[i] AS r, startNode(rels[i]) AS s, endNode(rels[i]) AS e
            RETURN DISTINCT s.id AS start, type(r) AS type, e.id AS end
            LIMIT 50
            """
            top_k = 20
        else:  # hop_count == 3
            # 3-hop: å¯å¤‰é•·ãƒ‘ã‚¹ [*1..3]
            query = """
            UNWIND $entities AS entity
            MATCH (n)
            WHERE n.id CONTAINS entity
            AND NOT n.id =~ '[0-9a-f]{32}'
            WITH collect(DISTINCT n) AS matched_nodes

            UNWIND matched_nodes AS start_node
            MATCH path = (start_node)-[*1..3]-(end_node)
            WHERE ALL(r IN relationships(path) WHERE type(r) <> 'MENTIONS')
            AND ALL(node IN nodes(path) WHERE NOT node.id =~ '[0-9a-f]{32}')
            AND start_node <> end_node

            WITH relationships(path) AS rels
            UNWIND range(0, size(rels)-1) AS i
            WITH rels[i] AS r, startNode(rels[i]) AS s, endNode(rels[i]) AS e
            RETURN DISTINCT s.id AS start, type(r) AS type, e.id AS end
            LIMIT 80
            """
            top_k = 25

        try:
            result = graph.query(query, params={"entities": entities})
            if result:
                # 4. LLMãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§é–¢é€£åº¦ã®é«˜ã„é–¢ä¿‚æ€§ã®ã¿ã«çµã‚‹
                result = rank_relations_by_relevance(question, result, top_k=top_k)
            return result if result else []
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å˜ç´”ãª1-hopãƒãƒƒãƒãƒ³ã‚°
            fallback_query = """
            MATCH (n)-[r]->(m)
            WHERE (
                ANY(entity IN $entities WHERE n.id CONTAINS entity OR m.id CONTAINS entity)
            )
            AND type(r) <> 'MENTIONS'
            AND NOT n.id =~ '[0-9a-f]{32}'
            AND NOT m.id =~ '[0-9a-f]{32}'
            RETURN DISTINCT n.id AS start, type(r) AS type, m.id AS end
            LIMIT 20
            """
            try:
                result = graph.query(fallback_query, params={"entities": entities})
                if result:
                    result = rank_relations_by_relevance(question, result, top_k=15)
                return result if result else []
            except Exception:
                return []

    # LCELãƒã‚§ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆGraph-First Retrievalï¼‰
    def retriever_and_merge(question: str):
        """ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’å„ªå…ˆã—ã€è£œåŠ©çš„ã«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ç”¨"""
        # 1. ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ã‚°ãƒ©ãƒ•æ¤œç´¢ã‚’å®Ÿè¡Œ
        triples = []
        enable_knowledge_graph = st.session_state.get('enable_knowledge_graph', True)

        if enable_knowledge_graph:
            triples = get_graph_context(question)

        # 2. ã‚°ãƒ©ãƒ•æ¤œç´¢çµæœãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’è£œåŠ©çš„ã«ä½¿ç”¨
        docs = []
        if triples:
            # ã‚°ãƒ©ãƒ•ã‹ã‚‰é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—ã—ã€ãã‚Œã«é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            entity_names = list(set([t.get('start') for t in triples] + [t.get('end') for t in triples]))

            # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ä»˜ãï¼‰
            if entity_names:
                chunk_query = """
                UNWIND $entity_names AS entity_name
                MATCH (e {id: entity_name})<-[:MENTIONS]-(chunk)
                WHERE chunk.id =~ '[0-9a-f]{32}'
                OPTIONAL MATCH (chunk)-[:FROM_DOCUMENT]->(doc:Document)
                RETURN DISTINCT chunk.id AS chunk_id, chunk.text AS text, doc.name AS source
                LIMIT 5
                """
                try:
                    chunk_results = graph.query(chunk_query, params={"entity_names": entity_names})
                    if chunk_results:
                        # ã‚°ãƒ©ãƒ•ã‹ã‚‰å–å¾—ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¿½åŠ ï¼ˆã‚½ãƒ¼ã‚¹æƒ…å ±ä»˜ãï¼‰
                        from langchain_core.documents import Document
                        docs = [Document(
                            page_content=r.get('text', ''),
                            metadata={
                                'id': r.get('chunk_id'),
                                'source': r.get('source', 'Unknown')
                            })
                            for r in chunk_results if r.get('text')]
                except Exception:
                    pass

        # 3. ã‚°ãƒ©ãƒ•ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒå–å¾—ã§ããªã„å ´åˆã¯ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’ä½¿ç”¨
        if not docs:
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ä½¿ç”¨ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
            if st.session_state.get('enable_japanese_search', False) and SUDACHI_AVAILABLE:
                try:
                    hybrid_retriever = HybridRetriever(PG_CONN, collection_name="graphrag")
                    query_embedding = embeddings.embed_query(question)
                    search_type = st.session_state.get('search_mode', 'hybrid')

                    # TopKå€¤ã‚’å–å¾—
                    retrieval_top_k = st.session_state.get('retrieval_top_k', 5)

                    hybrid_results = hybrid_retriever.search(
                        query_text=question,
                        query_vector=query_embedding,
                        k=retrieval_top_k,
                        search_type=search_type
                    )

                    # LangChain Documentå½¢å¼ã«å¤‰æ›
                    from langchain_core.documents import Document
                    docs = [
                        Document(
                            page_content=r['text'],
                            metadata=r['metadata']
                        ) for r in hybrid_results
                    ]
                except Exception as e:
                    st.warning(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰: {e}")
                    docs = vector_retriever.invoke(question)
            else:
                # å¾“æ¥ã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
                docs = vector_retriever.invoke(question)

        graph_lines = [
            f"{t.get('start')} -[{t.get('type')}]â†’ {t.get('end')}"
            for t in triples
        ] if triples else ["(ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãªã—)"]

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚ã‚‹
        doc_contexts = []
        for d in docs:
            source = d.metadata.get('source', 'Unknown')
            doc_contexts.append(f"[å‡ºå…¸: {source}]\n{d.page_content}")

        context = (
            "<GRAPH_CONTEXT>\n" + "\n".join(graph_lines) + "\n</GRAPH_CONTEXT>\n\n" +
            "<DOCUMENT_CONTEXT>\n" + "\n---\n".join(doc_contexts) + "\n</DOCUMENT_CONTEXT>"
        )
        return {
            "context": context,
            "question": question,
            "vector_sources": docs,
            "graph_sources": triples
        }

    prompt = PromptTemplate.from_template(
        """ã‚ãªãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚\nè³ªå•: {question}\n\n{context}\n\n---\nä¸Šè¨˜æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã«ã€æ—¥æœ¬èªã§ç¶²ç¾…çš„ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
è¤‡æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ãŸå ´åˆã¯ã€ãã‚Œãã‚Œã®å‡ºå…¸ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚"""
    )

    # LLMå‘¼ã³å‡ºã—éƒ¨åˆ†
    llm_chain = (
        prompt
        | create_chat_llm(temperature=0)
        | StrOutputParser()
    )

    # ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿æŒã™ã‚‹é–¢æ•°
    def generate_with_sources(data):
        answer = llm_chain.invoke({"question": data["question"], "context": data["context"]})
        return {
            "answer": answer,
            "vector_sources": data["vector_sources"],
            "graph_sources": data["graph_sources"]
        }

    chain = (
        RunnablePassthrough()
        | RunnableLambda(retriever_and_merge)
        | RunnableLambda(generate_with_sources)
    )

    return chain, graph

# ã‚°ãƒ©ãƒ•å–å¾—é–¢æ•°ï¼ˆæ”¹å–„ç‰ˆãƒ»ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å…±é€šï¼‰
def get_enhanced_graph_data(graph, limit=200):
    """ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‹ã‚‰æ‹¡å¼µã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒãƒ£ãƒ³ã‚¯IDé™¤å¤–ã€MENTIONSé–¢ä¿‚é™¤å¤–ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæƒ…å ±ä»˜ä¸ï¼‰"""
    # NetworkXã®å ´åˆã¯å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
    if hasattr(graph, 'get_graph_data'):
        # NetworkXGraph ã®å ´åˆ
        return graph.get_graph_data(limit=limit)

    # Neo4jã®å ´åˆã¯æ—¢å­˜ã®Cypherã‚¯ã‚¨ãƒª
    query = f"""
    MATCH (n)-[r]->(m)
    WHERE type(r) <> 'MENTIONS'
    AND NOT n.id =~ '[0-9a-f]{{32}}'
    AND NOT m.id =~ '[0-9a-f]{{32}}'
    OPTIONAL MATCH (n)<-[:MENTIONS]-(chunk_n)-[:FROM_DOCUMENT]->(doc_n:Document)
    OPTIONAL MATCH (m)<-[:MENTIONS]-(chunk_m)-[:FROM_DOCUMENT]->(doc_m:Document)
    WITH n, r, m, labels(n) as source_labels, labels(m) as target_labels,
         COLLECT(DISTINCT doc_n.name) AS source_docs,
         COLLECT(DISTINCT doc_m.name) AS target_docs
    RETURN
      n.id AS source,
      CASE WHEN size(source_labels) > 0 THEN source_labels[0] ELSE 'Unknown' END AS source_type,
      type(r) AS relation,
      m.id AS target,
      CASE WHEN size(target_labels) > 0 THEN target_labels[0] ELSE 'Unknown' END AS target_type,
      COUNT {{ (n)--() }} AS source_degree,
      COUNT {{ (m)--() }} AS target_degree,
      source_docs,
      target_docs
    LIMIT {limit}
    """
    result = graph.query(query)
    return result

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚
def get_graph_data(graph):
    """Neo4jã‹ã‚‰ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    return get_enhanced_graph_data(graph, limit=100)


def get_enhanced_subgraph_data(graph, center_nodes: List[str], hop: int = 1, limit: int = 500):
    """ã‚µãƒ–ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰åˆ¤å®šä»˜ãï¼‰"""
    backend = st.session_state.graph_backend

    if backend == "networkx":
        # NetworkXã®å ´åˆã¯å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨
        if hasattr(graph, 'get_subgraph_data'):
            return graph.get_subgraph_data(center_nodes, hop, limit)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: get_graph_data()ã§å…¨å–å¾—
            return graph.get_graph_data(limit=limit)
    elif backend == "neo4j":
        # Neo4jã®å ´åˆã¯ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æ¤œç´¢ä½¿ç”¨
        results = graph.query(params={'entities': center_nodes})
        # ç°¡æ˜“å¤‰æ›ï¼ˆNeo4jã®queryãƒ¡ã‚½ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’æƒ³å®šï¼‰
        graph_data = []
        for r in results:
            graph_data.append({
                'source': r.get('start', ''),
                'source_type': 'Unknown',
                'target': r.get('end', ''),
                'target_type': 'Unknown',
                'relation': r.get('type', 'RELATED'),
                'edge_key': 0,
                'source_degree': 0,
                'target_degree': 0,
                'source_docs': [],
                'target_docs': []
            })
        return graph_data[:limit]

    return []

# ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—æ¨è«–é–¢æ•°
def get_node_type(node_name: str, node_label: str = None) -> str:
    """ãƒãƒ¼ãƒ‰åã‚„ãƒ©ãƒ™ãƒ«ã‹ã‚‰ã‚¿ã‚¤ãƒ—ã‚’æ¨è«–"""
    if node_label and node_label != 'Unknown':
        return node_label

    # äººç‰©åˆ¤å®š
    person_keywords = ['å¤ªéƒ', 'å§«', 'çˆº', 'å©†', 'ç‹', 'ä¾', 'äºº', 'è€…']
    if any(kw in node_name for kw in person_keywords):
        return 'Person'

    # å ´æ‰€åˆ¤å®š
    place_keywords = ['å±±', 'å·', 'å³¶', 'æ‘', 'åŸ', 'å›½', 'éƒ½', 'é‡Œ']
    if any(kw in node_name for kw in place_keywords):
        return 'Place'

    # ã‚¤ãƒ™ãƒ³ãƒˆåˆ¤å®š
    event_keywords = ['æˆ¦', 'æ—…', 'é€€æ²»', 'ç™ºè¦‹', 'èª•ç”Ÿ', 'å‡ºä¼š']
    if any(kw in node_name for kw in event_keywords):
        return 'Event'

    # ç‰©åˆ¤å®š
    object_keywords = ['å®', 'åˆ€', 'èˆ¹', 'ç‰', 'ç®±', 'é¡']
    if any(kw in node_name for kw in object_keywords):
        return 'Object'

    return 'Other'

# ã‚¿ã‚¤ãƒ—ã”ã¨ã®è‰²ã‚’è¿”ã™
def get_color_for_type(node_type: str) -> str:
    """ãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè‰²ã‚’è¿”ã™"""
    color_map = {
        'Person': '#FF6B6B',      # èµ¤ç³»ï¼ˆäººç‰©ï¼‰
        'Place': '#4ECDC4',       # é’ç·‘ç³»ï¼ˆå ´æ‰€ï¼‰
        'Event': '#95E1D3',       # ç·‘ç³»ï¼ˆã‚¤ãƒ™ãƒ³ãƒˆï¼‰
        'Object': '#FFE66D',      # é»„è‰²ç³»ï¼ˆç‰©ï¼‰
        'Organization': '#A8E6CF', # è–„ç·‘ï¼ˆçµ„ç¹”ï¼‰
        'Other': '#95A5A6',       # ã‚°ãƒ¬ãƒ¼ï¼ˆãã®ä»–ï¼‰
        'Unknown': '#7F8C8D'      # æ¿ƒã„ã‚°ãƒ¬ãƒ¼ï¼ˆä¸æ˜ï¼‰
    }
    return color_map.get(node_type, '#95A5A6')

# Streamlit-Agraphå¯è¦–åŒ–é–¢æ•°
def visualize_graph_agraph(graph_data):
    """Streamlit-Agraphã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«å¯è¦–åŒ–"""
    try:
        from streamlit_agraph import agraph, Node, Edge, Config

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if not graph_data:
            st.warning("âš ï¸ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ˆAgraphï¼‰")
            return None

        nodes = []
        edges = []
        node_dict = {}

        # ãƒãƒ¼ãƒ‰åé›†ã¨ã‚¿ã‚¤ãƒ—åˆ¤å®š
        for item in graph_data:
            # å¿…é ˆã‚­ãƒ¼ã®æ¤œè¨¼
            if 'source' not in item or 'target' not in item or 'relation' not in item:
                st.warning(f"âš ï¸ ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ã‚¹ã‚­ãƒƒãƒ—: {item}")
                continue

            source_type = get_node_type(item['source'], item.get('source_type'))
            target_type = get_node_type(item['target'], item.get('target_type'))

            source_degree = item.get('source_degree', 1)
            target_degree = item.get('target_degree', 1)

            if item['source'] not in node_dict:
                node_dict[item['source']] = {
                    'type': source_type,
                    'degree': source_degree
                }

            if item['target'] not in node_dict:
                node_dict[item['target']] = {
                    'type': target_type,
                    'degree': target_degree
                }

        # ãƒãƒ¼ãƒ‰ä½œæˆï¼ˆã‚µã‚¤ã‚ºã‚’æ¥ç¶šæ•°ã«å¿œã˜ã¦æ§ãˆã‚ã«èª¿æ•´ï¼‰
        for node_id, node_info in node_dict.items():
            size = 8 + min(node_info['degree'] * 1.5, 20)  # æœ€å°8ã€æœ€å¤§28ï¼ˆæ§ãˆã‚ï¼‰
            color = get_color_for_type(node_info['type'])
            nodes.append(
                Node(
                    id=node_id,
                    label=node_id,
                    size=size,
                    color=color,
                    title=f"{node_id} ({node_info['type']}, æ¥ç¶šæ•°: {node_info['degree']})"
                )
            )

        # ã‚¨ãƒƒã‚¸ä½œæˆ
        for item in graph_data:
            if 'source' in item and 'target' in item and 'relation' in item:
                edges.append(
                    Edge(
                        source=item['source'],
                        target=item['target'],
                        label=item['relation'],
                        color="#888888"
                    )
                )

        # ãƒãƒ¼ãƒ‰ã¾ãŸã¯ã‚¨ãƒƒã‚¸ãŒç©ºã®å ´åˆ
        if not nodes or not edges:
            st.warning(f"âš ï¸ Agraphãƒ‡ãƒ¼ã‚¿ä¸è¶³: ãƒãƒ¼ãƒ‰{len(nodes)}å€‹ã€ã‚¨ãƒƒã‚¸{len(edges)}æœ¬")
            return None

        # è¨­å®š
        config = Config(
            width="100%",
            height=700,
            directed=True,
            nodeHighlightBehavior=True,
            highlightColor="#F7B731",
            collapsible=True,
            node={'labelProperty': 'label'},
            link={'labelProperty': 'label', 'renderLabel': True}
        )

        agraph(nodes=nodes, edges=edges, config=config)
        return True  # æˆåŠŸæ™‚ã¯Trueã‚’è¿”ã™

    except ImportError:
        st.info("â„¹ï¸ streamlit-agraphãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None
    except Exception as e:
        st.warning(f"âš ï¸ Agraphå¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        import traceback
        st.code(traceback.format_exc(), language="python")
        return None

# Pyviså¼·åŒ–ç‰ˆå¯è¦–åŒ–é–¢æ•°
def visualize_graph_pyvis_enhanced(graph_data):
    """Pyvisã§å¼·åŒ–ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–"""
    try:
        from pyvis.network import Network

        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if not graph_data:
            st.warning("âš ï¸ ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ï¼ˆPyvisï¼‰")
            return None

        net = Network(
            height="700px",
            width="100%",
            bgcolor="#1a1a1a",
            font_color="white",
            notebook=False
        )

        # ç‰©ç†ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
        net.set_options("""
        {
          "physics": {
            "enabled": true,
            "barnesHut": {
              "gravitationalConstant": -8000,
              "springLength": 250,
              "springConstant": 0.001,
              "damping": 0.5
            },
            "minVelocity": 0.75
          },
          "nodes": {
            "font": {"size": 14, "face": "arial"},
            "borderWidth": 2,
            "borderWidthSelected": 4
          },
          "edges": {
            "color": {"inherit": "from"},
            "smooth": {"type": "continuous"},
            "font": {"size": 12, "align": "middle"}
          },
          "interaction": {
            "hover": true,
            "navigationButtons": true,
            "keyboard": true
          }
        }
        """)

        node_dict = {}

        # ãƒãƒ¼ãƒ‰æƒ…å ±åé›†
        for item in graph_data:
            # å¿…é ˆã‚­ãƒ¼ã®æ¤œè¨¼
            if 'source' not in item or 'target' not in item or 'relation' not in item:
                st.warning(f"âš ï¸ ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ã‚¹ã‚­ãƒƒãƒ—: {item}")
                continue

            source_type = get_node_type(item['source'], item.get('source_type'))
            target_type = get_node_type(item['target'], item.get('target_type'))

            source_degree = item.get('source_degree', 1)
            target_degree = item.get('target_degree', 1)
            source_docs = item.get('source_docs', [])
            target_docs = item.get('target_docs', [])

            if item['source'] not in node_dict:
                node_dict[item['source']] = {
                    'type': source_type,
                    'degree': source_degree,
                    'color': get_color_for_type(source_type),
                    'docs': source_docs
                }

            if item['target'] not in node_dict:
                node_dict[item['target']] = {
                    'type': target_type,
                    'degree': target_degree,
                    'color': get_color_for_type(target_type),
                    'docs': target_docs
                }

        # ãƒãƒ¼ãƒ‰è¿½åŠ ï¼ˆã‚µã‚¤ã‚ºã‚’æ§ãˆã‚ã«èª¿æ•´ï¼‰
        for node_id, node_info in node_dict.items():
            size = 12 + min(node_info['degree'] * 1, 18)  # æœ€å°12ã€æœ€å¤§30ï¼ˆæ§ãˆã‚ï¼‰
            docs_str = "<br>å‡ºå…¸: " + ", ".join(node_info['docs']) if node_info.get('docs') else ""
            net.add_node(
                node_id,
                label=node_id,
                color=node_info['color'],
                size=size,
                title=f"<b>{node_id}</b><br>ã‚¿ã‚¤ãƒ—: {node_info['type']}<br>æ¥ç¶šæ•°: {node_info['degree']}{docs_str}",
                borderWidth=2
            )

        # ã‚¨ãƒƒã‚¸è¿½åŠ 
        for item in graph_data:
            if 'source' in item and 'target' in item and 'relation' in item:
                net.add_edge(
                    item['source'],
                    item['target'],
                    label=item['relation'],
                    title=item['relation'],
                    arrows='to',
                    color='#666666'
                )

        # ãƒãƒ¼ãƒ‰ã¾ãŸã¯ã‚¨ãƒƒã‚¸ãŒç©ºã®å ´åˆ
        if len(node_dict) == 0:
            st.warning("âš ï¸ Pyvisãƒ‡ãƒ¼ã‚¿ä¸è¶³: ãƒãƒ¼ãƒ‰ãŒ0å€‹ã§ã™")
            return None

        net.save_graph("graph_enhanced.html")
        with open("graph_enhanced.html", "r", encoding="utf-8") as f:
            html = f.read()
        return html

    except ImportError:
        st.info("â„¹ï¸ pyvisãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None
    except Exception as e:
        st.warning(f"âš ï¸ Pyviså¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {type(e).__name__}: {e}")
        import traceback
        st.code(traceback.format_exc(), language="python")
        return None

# æ—§ã‚°ãƒ©ãƒ•å¯è¦–åŒ–é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
def visualize_graph(graph_data):
    """pyvisã§ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    return visualize_graph_pyvis_enhanced(graph_data)

# è‡ªç„¶è¨€èªâ†’Cypherã‚¯ã‚¨ãƒªå¤‰æ›é–¢æ•°
def natural_language_to_cypher(query: str) -> str:
    """è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’Cypherã‚¯ã‚¨ãƒªã«å¤‰æ›"""
    try:
        llm = create_chat_llm(temperature=0)

        prompt = f"""ã‚ãªãŸã¯Neo4jã®Cypherã‚¯ã‚¨ãƒªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®è‡ªç„¶è¨€èªã‚’Cypherã‚¯ã‚¨ãƒªã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

ã€ã‚°ãƒ©ãƒ•ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã€‘
- ãƒãƒ¼ãƒ‰: ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã¯ `id` (ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’æ ¼ç´)
- ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—: å‹•çš„ï¼ˆMENTIONSä»¥å¤–ã®ã™ã¹ã¦ã®é–¢ä¿‚ã‚¿ã‚¤ãƒ—ï¼‰
- é™¤å¤–æ¡ä»¶: ãƒãƒ£ãƒ³ã‚¯ãƒãƒ¼ãƒ‰ï¼ˆid =~ '[0-9a-f]{{32}}'ï¼‰ã¯é™¤å¤–ã™ã‚‹ã“ã¨
- MENTIONSé–¢ä¿‚ã¯é™¤å¤–ã™ã‚‹ã“ã¨

ã€ã‚¯ã‚¨ãƒªä½œæˆãƒ«ãƒ¼ãƒ«ã€‘
1. RETURNå¥ã§å¿…ãšä»¥ä¸‹ã‚’è¿”ã™ã“ã¨:
   - ãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ã®å ´åˆ: n.id AS source, type(r) AS relation, m.id AS target
   - ãƒãƒ¼ãƒ‰ã®ã¿ã®å ´åˆ: n.id AS node_id, labels(n) AS labels
2. ãƒãƒ£ãƒ³ã‚¯ãƒãƒ¼ãƒ‰ã‚’é™¤å¤–: WHERE NOT n.id =~ '[0-9a-f]{{32}}'
3. MENTIONSé–¢ä¿‚ã‚’é™¤å¤–: WHERE type(r) <> 'MENTIONS'
4. LIMITå¥ã‚’å¿…ãšä»˜ä¸ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50ï¼‰

è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒª: {query}

Cypherã‚¯ã‚¨ãƒªï¼ˆã‚¯ã‚¨ãƒªã®ã¿å‡ºåŠ›ã€èª¬æ˜ä¸è¦ï¼‰:"""

        response = llm.invoke(prompt)
        cypher_query = response.content.strip()

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»ï¼ˆ```cypher ``` ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
        if cypher_query.startswith("```"):
            lines = cypher_query.split("\n")
            cypher_query = "\n".join(lines[1:-1]) if len(lines) > 2 else cypher_query

        return cypher_query

    except Exception as e:
        st.error(f"Cypherã‚¯ã‚¨ãƒªå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return ""

# Cypherã‚¯ã‚¨ãƒªå®Ÿè¡Œ&å¯è¦–åŒ–é–¢æ•°
def execute_cypher_and_visualize(cypher_query: str, graph):
    """Cypherã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    try:
        # å±é™ºãªã‚¯ã‚¨ãƒªã‚’æ¤œå‡º
        dangerous_keywords = ['DELETE', 'DROP', 'CREATE', 'MERGE', 'SET', 'REMOVE', 'DETACH']
        upper_query = cypher_query.upper()

        for keyword in dangerous_keywords:
            if keyword in upper_query:
                st.error(f"âš ï¸ å±é™ºãªã‚¯ã‚¨ãƒªãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {keyword} ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“")
                return None

        # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        result = graph.query(cypher_query)

        if not result:
            st.warning("ã‚¯ã‚¨ãƒªçµæœãŒç©ºã§ã™")
            return None

        return result

    except Exception as e:
        st.error(f"ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        st.code(traceback.format_exc(), language="python")
        return None

# ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºé–¢æ•°ï¼ˆç·¨é›†æ©Ÿèƒ½ä»˜ãï¼‰
def display_data_tables(graph_data, graph=None, enable_edit=False):
    """ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§è¡¨ç¤ºï¼ˆç·¨é›†æ©Ÿèƒ½ä»˜ãï¼‰"""
    import pandas as pd

    # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ
    nodes_dict = {}
    for item in graph_data:
        # ã‚½ãƒ¼ã‚¹ãƒãƒ¼ãƒ‰
        if item['source'] not in nodes_dict:
            source_type = get_node_type(item['source'], item.get('source_type'))
            nodes_dict[item['source']] = {
                'ãƒãƒ¼ãƒ‰ID': item['source'],
                'ã‚¿ã‚¤ãƒ—': source_type,
                'æ¥ç¶šæ•°': item.get('source_degree', 0),
                'è‰²': get_color_for_type(source_type)
            }

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒ¼ãƒ‰
        if item['target'] not in nodes_dict:
            target_type = get_node_type(item['target'], item.get('target_type'))
            nodes_dict[item['target']] = {
                'ãƒãƒ¼ãƒ‰ID': item['target'],
                'ã‚¿ã‚¤ãƒ—': target_type,
                'æ¥ç¶šæ•°': item.get('target_degree', 0),
                'è‰²': get_color_for_type(target_type)
            }

    # ã‚¨ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    edges_list = []
    for item in graph_data:
        edges_list.append({
            'å§‹ç‚¹': item['source'],
            'ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³': item['relation'],
            'çµ‚ç‚¹': item['target'],
            'edge_key': item.get('edge_key', 0)
        })

    # ãƒãƒ¼ãƒ‰ãƒ†ãƒ¼ãƒ–ãƒ«
    st.subheader("ğŸ“ ãƒãƒ¼ãƒ‰ä¸€è¦§")

    # ç·¨é›†æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã¯ç·¨é›†ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
    if enable_edit and graph:
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("â• æ–°è¦ãƒãƒ¼ãƒ‰è¿½åŠ ", key="add_node_btn"):
                st.session_state.edit_mode = "add_node"

        # ç·¨é›†ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
        if st.session_state.get('edit_mode') == 'add_node':
            with st.expander("â• æ–°è¦ãƒãƒ¼ãƒ‰è¿½åŠ ", expanded=True):
                edit_node_dialog(graph, None)
                if st.button("é–‰ã˜ã‚‹"):
                    st.session_state.edit_mode = None
                    st.rerun()

    nodes_df = pd.DataFrame(list(nodes_dict.values()))
    st.dataframe(
        nodes_df.sort_values('æ¥ç¶šæ•°', ascending=False),
        width='stretch',
        hide_index=True
    )

    # ç·¨é›†æ©Ÿèƒ½: ãƒãƒ¼ãƒ‰å€‹åˆ¥ç·¨é›†ãƒ»å‰Šé™¤
    if enable_edit and graph:
        st.caption("ãƒãƒ¼ãƒ‰ã‚’ç·¨é›†ãƒ»å‰Šé™¤ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„")
        selected_node = st.selectbox(
            "ãƒãƒ¼ãƒ‰ã‚’é¸æŠ",
            options=[""] + list(nodes_dict.keys()),
            key="selected_node"
        )

        if selected_node:
            col1, col2 = st.columns(2)
            with col1:
                if st.button("âœï¸ ç·¨é›†", key=f"edit_node_{selected_node}"):
                    st.session_state.editing_node = selected_node

            # ç·¨é›†ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯å¸¸ã«ãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤º
            if st.session_state.get('editing_node') == selected_node:
                node_info = graph.get_node_info(selected_node)
                if node_info:
                    with st.expander(f"âœï¸ ãƒãƒ¼ãƒ‰ç·¨é›†: {selected_node}", expanded=True):
                        edit_node_dialog(graph, node_info)
            with col2:
                if st.button("ğŸ—‘ï¸ å‰Šé™¤", key=f"delete_node_{selected_node}"):
                    if st.session_state.get(f'confirm_delete_node_{selected_node}'):
                        success = graph.delete_node(selected_node)
                        if success:
                            st.success(f"âœ… ãƒãƒ¼ãƒ‰ '{selected_node}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + å³åº§ã«å†å–å¾—
                            graph_data = graph.get_graph_data(limit=200)
                            st.session_state.graph_data_cache = graph_data
                            st.rerun()
                        else:
                            st.error("å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
                        st.session_state[f'confirm_delete_node_{selected_node}'] = False
                    else:
                        st.session_state[f'confirm_delete_node_{selected_node}'] = True
                        st.warning(f"âš ï¸ ãƒãƒ¼ãƒ‰ '{selected_node}' ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿã‚‚ã†ä¸€åº¦å‰Šé™¤ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv_nodes = nodes_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ãƒãƒ¼ãƒ‰ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_nodes,
        file_name="nodes.csv",
        mime="text/csv"
    )

    st.markdown("---")

    # ã‚¨ãƒƒã‚¸ãƒ†ãƒ¼ãƒ–ãƒ«
    st.subheader("ğŸ”— ã‚¨ãƒƒã‚¸ä¸€è¦§")

    # ç·¨é›†æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã¯è¿½åŠ ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
    if enable_edit and graph:
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("â• æ–°è¦ã‚¨ãƒƒã‚¸è¿½åŠ ", key="add_edge_btn"):
                st.session_state.edit_mode = "add_edge"

        # ç·¨é›†ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
        if st.session_state.get('edit_mode') == 'add_edge':
            all_node_ids = list(nodes_dict.keys())
            with st.expander("â• æ–°è¦ã‚¨ãƒƒã‚¸è¿½åŠ ", expanded=True):
                edit_edge_dialog(graph, None, all_node_ids)
                if st.button("é–‰ã˜ã‚‹", key="close_add_edge"):
                    st.session_state.edit_mode = None
                    st.rerun()

    edges_df = pd.DataFrame(edges_list)
    st.dataframe(
        edges_df,
        width='stretch',
        hide_index=True
    )

    # ç·¨é›†æ©Ÿèƒ½: ã‚¨ãƒƒã‚¸å€‹åˆ¥ç·¨é›†ãƒ»å‰Šé™¤
    if enable_edit and graph:
        st.caption("ã‚¨ãƒƒã‚¸ã‚’ç·¨é›†ãƒ»å‰Šé™¤ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„")

        # ã‚¨ãƒƒã‚¸é¸æŠè‚¢ã‚’ä½œæˆ
        edge_options = [""] + [f"{e['å§‹ç‚¹']} â†’ {e['çµ‚ç‚¹']} ({e['ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³']})" for e in edges_list]
        selected_edge_str = st.selectbox(
            "ã‚¨ãƒƒã‚¸ã‚’é¸æŠ",
            options=edge_options,
            key="selected_edge"
        )

        if selected_edge_str:
            # é¸æŠã•ã‚ŒãŸã‚¨ãƒƒã‚¸ã‚’è§£æ
            selected_idx = edge_options.index(selected_edge_str) - 1
            if selected_idx >= 0:
                selected_edge_data = edges_list[selected_idx]
                source = selected_edge_data['å§‹ç‚¹']
                target = selected_edge_data['çµ‚ç‚¹']
                rel_type = selected_edge_data['ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³']
                edge_key = selected_edge_data.get('edge_key', 0)

                col1, col2 = st.columns(2)
                with col1:
                    if st.button("âœï¸ ç·¨é›†", key=f"edit_edge_{selected_idx}"):
                        st.session_state.editing_edge = selected_idx

                # ç·¨é›†ãƒ¢ãƒ¼ãƒ‰æ™‚ã¯å¸¸ã«ãƒ€ã‚¤ã‚¢ãƒ­ã‚°è¡¨ç¤º
                if st.session_state.get('editing_edge') == selected_idx:
                    edge_info = graph.get_edge_info(source, target, edge_key)
                    if edge_info:
                        with st.expander(f"âœï¸ ã‚¨ãƒƒã‚¸ç·¨é›†: {source} â†’ {target}", expanded=True):
                            edit_edge_dialog(graph, edge_info)
                with col2:
                    if st.button("ğŸ—‘ï¸ å‰Šé™¤", key=f"delete_edge_{selected_idx}"):
                        if st.session_state.get(f'confirm_delete_edge_{selected_idx}'):
                            success = graph.delete_edge(source, target, edge_key)
                            if success:
                                st.success(f"âœ… ã‚¨ãƒƒã‚¸ '{source} â†’ {target}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + å³åº§ã«å†å–å¾—
                                graph_data = graph.get_graph_data(limit=200)
                                st.session_state.graph_data_cache = graph_data
                                st.rerun()
                            else:
                                st.error("å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
                            st.session_state[f'confirm_delete_edge_{selected_idx}'] = False
                        else:
                            st.session_state[f'confirm_delete_edge_{selected_idx}'] = True
                            st.warning(f"âš ï¸ ã‚¨ãƒƒã‚¸ '{source} â†’ {target}' ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿã‚‚ã†ä¸€åº¦å‰Šé™¤ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    csv_edges = edges_df.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ã‚¨ãƒƒã‚¸ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv_edges,
        file_name="edges.csv",
        mime="text/csv"
    )

    # çµ±è¨ˆæƒ…å ±
    st.markdown("---")
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ç·ãƒãƒ¼ãƒ‰æ•°", len(nodes_dict))
    with col2:
        st.metric("ç·ã‚¨ãƒƒã‚¸æ•°", len(edges_list))
    with col3:
        avg_degree = sum(n['æ¥ç¶šæ•°'] for n in nodes_dict.values()) / len(nodes_dict) if nodes_dict else 0
        st.metric("å¹³å‡æ¥ç¶šæ•°", f"{avg_degree:.1f}")


# ã‚°ãƒ©ãƒ•ç·¨é›†ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def edit_node_dialog(graph, node_info=None):
    """ãƒãƒ¼ãƒ‰ç·¨é›†ãƒ€ã‚¤ã‚¢ãƒ­ã‚°"""
    st.subheader("âœï¸ ãƒãƒ¼ãƒ‰ç·¨é›†" if node_info else "â• æ–°è¦ãƒãƒ¼ãƒ‰è¿½åŠ ")

    with st.form("node_form"):
        if node_info:
            node_id = st.text_input("ãƒãƒ¼ãƒ‰ID", value=node_info['id'], disabled=True)
            node_type = st.text_input("ã‚¿ã‚¤ãƒ—", value=node_info.get('type', 'Unknown'))
            properties_str = st.text_area(
                "ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ (JSONå½¢å¼)",
                value=json.dumps(node_info.get('properties', {}), ensure_ascii=False, indent=2)
            )
        else:
            node_id = st.text_input("ãƒãƒ¼ãƒ‰ID", placeholder="ä¾‹: æ¡ƒå¤ªéƒ")
            node_type = st.text_input("ã‚¿ã‚¤ãƒ—", value="Unknown", placeholder="ä¾‹: Person")
            properties_str = st.text_area("ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ (JSONå½¢å¼)", value="{}")

        col1, col2 = st.columns(2)
        with col1:
            submit = st.form_submit_button("ğŸ’¾ ä¿å­˜", type="primary")
        with col2:
            cancel = st.form_submit_button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«")

        if submit:
            try:
                properties = json.loads(properties_str) if properties_str.strip() else {}

                if node_info:
                    # æ›´æ–°
                    success = graph.update_node(node_id, node_type, properties)
                    if success:
                        st.success(f"âœ… ãƒãƒ¼ãƒ‰ '{node_id}' ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
                        # ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
                        st.session_state.editing_node = None
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + å³åº§ã«å†å–å¾—
                        graph_data = graph.get_graph_data(limit=200)
                        st.session_state.graph_data_cache = graph_data
                        st.rerun()
                    else:
                        st.error("æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    # æ–°è¦è¿½åŠ 
                    if not node_id:
                        st.error("ãƒãƒ¼ãƒ‰IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
                    else:
                        success = graph.add_node_manual(node_id, node_type, properties)
                        if success:
                            st.success(f"âœ… ãƒãƒ¼ãƒ‰ '{node_id}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                            # ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ï¼ˆæ–°è¦è¿½åŠ ã®å ´åˆã¯è©²å½“ãªã—ï¼‰
                            st.session_state.edit_mode = None
                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + å³åº§ã«å†å–å¾—
                            graph_data = graph.get_graph_data(limit=200)
                            st.session_state.graph_data_cache = graph_data
                            st.rerun()
                        else:
                            st.error("è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ")
            except json.JSONDecodeError:
                st.error("ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™")

        if cancel:
            # ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            st.session_state.editing_node = None
            st.session_state.edit_mode = None
            st.rerun()


def edit_edge_dialog(graph, edge_info=None, all_nodes=None):
    """ã‚¨ãƒƒã‚¸ç·¨é›†ãƒ€ã‚¤ã‚¢ãƒ­ã‚°"""
    st.subheader("âœï¸ ã‚¨ãƒƒã‚¸ç·¨é›†" if edge_info else "â• æ–°è¦ã‚¨ãƒƒã‚¸è¿½åŠ ")

    if all_nodes is None:
        all_nodes = []

    with st.form("edge_form"):
        if edge_info:
            source = st.text_input("å§‹ç‚¹ãƒãƒ¼ãƒ‰", value=edge_info['source'], disabled=True)
            target = st.text_input("çµ‚ç‚¹ãƒãƒ¼ãƒ‰", value=edge_info['target'], disabled=True)
            edge_key = edge_info.get('edge_key', 0)
            rel_type = st.text_input("ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—", value=edge_info.get('type', 'RELATED'))
            properties_str = st.text_area(
                "ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ (JSONå½¢å¼)",
                value=json.dumps(edge_info.get('properties', {}), ensure_ascii=False, indent=2)
            )
        else:
            if all_nodes:
                source = st.selectbox("å§‹ç‚¹ãƒãƒ¼ãƒ‰", options=all_nodes)
                target = st.selectbox("çµ‚ç‚¹ãƒãƒ¼ãƒ‰", options=all_nodes)
            else:
                source = st.text_input("å§‹ç‚¹ãƒãƒ¼ãƒ‰", placeholder="ä¾‹: æ¡ƒå¤ªéƒ")
                target = st.text_input("çµ‚ç‚¹ãƒãƒ¼ãƒ‰", placeholder="ä¾‹: é¬¼")
            edge_key = 0
            rel_type = st.text_input("ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—", value="RELATED", placeholder="ä¾‹: å€’ã—ãŸ")
            properties_str = st.text_area("ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ (JSONå½¢å¼)", value="{}")

        col1, col2 = st.columns(2)
        with col1:
            submit = st.form_submit_button("ğŸ’¾ ä¿å­˜", type="primary")
        with col2:
            cancel = st.form_submit_button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«")

        if submit:
            try:
                properties = json.loads(properties_str) if properties_str.strip() else {}

                if edge_info:
                    # æ›´æ–°
                    success = graph.update_edge(source, target, edge_key, rel_type, properties)
                    if success:
                        st.success(f"âœ… ã‚¨ãƒƒã‚¸ '{source} -> {target}' ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
                        # ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
                        st.session_state.editing_edge = None
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + å³åº§ã«å†å–å¾—
                        graph_data = graph.get_graph_data(limit=200)
                        st.session_state.graph_data_cache = graph_data
                        st.rerun()
                    else:
                        st.error("æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    # æ–°è¦è¿½åŠ 
                    if not source or not target:
                        st.error("å§‹ç‚¹ã¨çµ‚ç‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
                    else:
                        edge_key = graph.add_edge_manual(source, target, rel_type, properties)
                        if edge_key is not None:
                            st.success(f"âœ… ã‚¨ãƒƒã‚¸ '{source} -> {target}' ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                            # ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢ï¼ˆæ–°è¦è¿½åŠ ã®å ´åˆã¯è©²å½“ãªã—ï¼‰
                            st.session_state.edit_mode = None
                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ + å³åº§ã«å†å–å¾—
                            graph_data = graph.get_graph_data(limit=200)
                            st.session_state.graph_data_cache = graph_data
                            st.rerun()
                        else:
                            st.error("è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ")
            except json.JSONDecodeError:
                st.error("ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®JSONå½¢å¼ãŒä¸æ­£ã§ã™")

        if cancel:
            # ç·¨é›†çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            st.session_state.editing_edge = None
            st.session_state.edit_mode = None
            st.rerun()


def confirm_delete_dialog(item_type, item_name, callback):
    """å‰Šé™¤ç¢ºèªãƒ€ã‚¤ã‚¢ãƒ­ã‚°"""
    st.warning(f"âš ï¸ {item_type} '{item_name}' ã‚’å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿ")
    st.caption("ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ã€‚")

    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ å‰Šé™¤ã™ã‚‹", type="primary"):
            if callback():
                st.success(f"âœ… {item_type} '{item_name}' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                st.session_state.graph_data_cache = None  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
                st.rerun()
            else:
                st.error("å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")
    with col2:
        if st.button("âŒ ã‚­ãƒ£ãƒ³ã‚»ãƒ«"):
            st.rerun()

# ãƒ¡ã‚¤ãƒ³UI
st.header("ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

# æ—¢å­˜ã‚°ãƒ©ãƒ•ã®ãƒã‚§ãƒƒã‚¯ï¼ˆåˆå›ã®ã¿ï¼‰
if not st.session_state.existing_graph_loaded and not st.session_state.initialized:
    try:
        # ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æ¥ç¶š
        if st.session_state.graph_backend == "neo4j":
            temp_graph = Neo4jGraph(
                url=NEO4J_URI,
                username=NEO4J_USER,
                password=NEO4J_PW,
                enhanced_schema=False  # APOCä¸è¦
            )
        else:  # networkx
            from networkx_graph import NetworkXGraph
            temp_graph = NetworkXGraph(storage_path="graph.pkl", auto_save=True)

        graph_info = check_existing_graph(temp_graph, st.session_state.graph_backend)

        if graph_info['exists']:
            st.info(f"ğŸ“Š æ—¢å­˜ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ: ãƒãƒ¼ãƒ‰ {graph_info['node_count']}å€‹ã€ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ— {graph_info['rel_count']}æœ¬")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ æ—¢å­˜ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã‚€", type="primary"):
                    with st.spinner("æ—¢å­˜ã‚°ãƒ©ãƒ•ã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ã‚’å¾©å…ƒä¸­..."):
                        try:
                            st.session_state.chain, st.session_state.graph = restore_from_existing_graph()
                            st.session_state.initialized = True
                            st.session_state.existing_graph_loaded = True
                            st.success("âœ… æ—¢å­˜ã‚°ãƒ©ãƒ•ã‹ã‚‰å¾©å…ƒå®Œäº†ï¼ã™ãã«è³ªå•ã§ãã¾ã™ã€‚")
                            st.rerun()
                        except Exception as e:
                            st.error(f"å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")

            with col2:
                if st.button("ğŸ—‘ï¸ æ—¢å­˜ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªã‚¢ã—ã¦æ–°è¦ä½œæˆ"):
                    with st.spinner("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ä¸­..."):
                        try:
                            temp_graph.query("MATCH (n) DETACH DELETE n")
                            st.session_state.existing_graph_loaded = True
                            st.success("âœ… ã‚¯ãƒªã‚¢å®Œäº†ã€‚æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                            st.rerun()
                        except Exception as e:
                            st.error(f"ã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}")

            st.markdown("---")
    except Exception as e:
        # Neo4jæ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆå¾Œç¶šã®å‡¦ç†ã§å¯¾å¿œï¼‰
        pass

uploaded_files = st.file_uploader(
    "PDF/ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
    type=["pdf", "txt"],
    accept_multiple_files=True,
    help="è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™"
)
csv_edges_file = st.file_uploader(
    "edges.csv (source,target,label)",
    type=["csv"],
    accept_multiple_files=False,
    help="ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸é–¢ä¿‚ã‚’CSVã§è¿½åŠ ã™ã‚‹å ´åˆã«æŒ‡å®šã—ã¦ãã ã•ã„"
)
has_docs = bool(uploaded_files)
has_csv = bool(csv_edges_file)

if has_docs:
    st.success(f"âœ… {len(uploaded_files)} ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã—ãŸ")

    with st.expander("ğŸ“„ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«"):
        for file in uploaded_files:
            st.write(f"- {file.name} ({file.size} bytes)")

if has_csv:
    st.info(f"ğŸ”— edges.csv ã‚’å—ä¿¡: {csv_edges_file.name}")

# ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ãƒœã‚¿ãƒ³ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¾ãŸã¯CSVãŒã‚ã‚Œã°è¡¨ç¤ºï¼‰
if has_docs or has_csv:
    if st.button("ğŸš€ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰", type="primary"):
        source_docs = []
        if has_docs:
            with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ä¸­..."):
                try:
                    source_docs = load_documents(uploaded_files)
                    total_chars = sum(len(doc.page_content) for doc in source_docs)
                    st.info(f"ğŸ“„ {len(source_docs)} ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†ï¼ˆç·æ–‡å­—æ•°: {total_chars:,} æ–‡å­—ï¼‰")
                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    st.stop()

        with st.spinner("ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ä¸­... (æ•°åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)"):
            try:
                csv_edges = load_csv_edges(csv_edges_file) if has_csv else []
                st.session_state.chain, st.session_state.graph = build_rag_system(source_docs, csv_edges)
                st.session_state.initialized = True
                st.session_state.uploaded_files = [f.name for f in uploaded_files] if has_docs else []
                # æ–°ã—ã„ã‚°ãƒ©ãƒ•ã«åˆã‚ã›ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
                st.session_state.graph_data_cache = None
                if 'all_node_list' in st.session_state:
                    st.session_state.all_node_list = None
                st.success("âœ… ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†!")
            except Exception as e:
                st.error(f"æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                st.code(traceback.format_exc())

st.markdown("---")

# ã‚¿ãƒ–å½¢å¼UI
tab1, tab2 = st.tabs(["ğŸ’¬ è³ªå•å¿œç­”", "ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•æ¢ç´¢"])

with tab1:
    st.header("ğŸ’¬ è³ªå•å¿œç­”")

    # è³ªå•å…¥åŠ›
    if st.session_state.initialized:
        question = st.text_area("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", height=150, key="question_input")

        if st.button("ğŸ” è³ªå•ã™ã‚‹", type="primary"):
            if question:
                with st.spinner("å›ç­”ç”Ÿæˆä¸­..."):
                    try:
                        result = st.session_state.chain.invoke(question)

                        # å›ç­”è¡¨ç¤º
                        st.markdown("### ğŸ“ å›ç­”")
                        st.markdown(result["answer"])

                        # å¼•ç”¨å…ƒ: Vector RAG
                        with st.expander("ğŸ“š å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (Vector RAG)", expanded=False):
                            vector_sources = result.get("vector_sources", [])
                            if vector_sources:
                                for i, doc in enumerate(vector_sources, 1):
                                    st.markdown(f"**ãƒãƒ£ãƒ³ã‚¯ {i}:**")
                                    st.text(doc.page_content)
                                    if i < len(vector_sources):
                                        st.divider()
                            else:
                                st.info("ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢çµæœãªã—")

                        # å¼•ç”¨å…ƒ: Graph RAG
                        with st.expander("ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ• (Graph RAG)", expanded=False):
                            graph_sources = result.get("graph_sources", [])
                            if graph_sources:
                                for triple in graph_sources:
                                    st.markdown(f"- `{triple.get('start')}` -[{triple.get('type')}]â†’ `{triple.get('end')}`")
                            else:
                                st.info("ã‚°ãƒ©ãƒ•æ¤œç´¢çµæœãªã—")

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    else:
        st.info("ã¾ãšRAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„")

with tab2:
    st.header("ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•æ¢ç´¢")

    if st.session_state.initialized:
        # è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰é¸æŠ
        display_mode = st.radio(
            "è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰",
            ["ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•å¯è¦–åŒ–", "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«", "ğŸ” Cypherã‚¯ã‚¨ãƒªæ¤œç´¢"],
            horizontal=True
        )

        st.markdown("---")

        # ãƒ¢ãƒ¼ãƒ‰1: ã‚°ãƒ©ãƒ•å¯è¦–åŒ–
        if display_mode == "ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•å¯è¦–åŒ–":
            if not show_graph:
                st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã€ŒãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤ºã€ã‚’ONã«ã—ã¦ãã ã•ã„")
            else:
                # å¯è¦–åŒ–ç¯„å›²é¸æŠ
                viz_scope = st.radio(
                    "ğŸ“Š å¯è¦–åŒ–ç¯„å›²",
                    ["å…¨ä½“è¡¨ç¤º", "éƒ¨åˆ†è¡¨ç¤ºï¼ˆæ¤œç´¢ï¼‰"],
                    horizontal=True,
                    help="å¤§è¦æ¨¡ã‚°ãƒ©ãƒ•ã®å ´åˆã¯éƒ¨åˆ†è¡¨ç¤ºã‚’æ¨å¥¨ã—ã¾ã™"
                )

                if viz_scope == "éƒ¨åˆ†è¡¨ç¤ºï¼ˆæ¤œç´¢ï¼‰":
                    # éƒ¨åˆ†å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰
                    st.markdown("### ğŸ” ãƒãƒ¼ãƒ‰æ¤œç´¢")

                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
                    if 'center_nodes' not in st.session_state:
                        st.session_state.center_nodes = []

                    # å…¨ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆå–å¾—ï¼ˆåˆå›ã®ã¿ï¼‰
                    if 'all_node_list' not in st.session_state:
                        if st.session_state.graph_data_cache:
                            graph_data = st.session_state.graph_data_cache
                            all_nodes = list(set([item['source'] for item in graph_data] + [item['target'] for item in graph_data]))
                            st.session_state.all_node_list = sorted(all_nodes)
                        else:
                            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ä¸€åº¦å–å¾—
                            with st.spinner("ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆå–å¾—ä¸­..."):
                                try:
                                    graph_data = get_enhanced_graph_data(st.session_state.graph, limit=max_nodes)
                                    st.session_state.graph_data_cache = graph_data
                                    all_nodes = list(set([item['source'] for item in graph_data] + [item['target'] for item in graph_data]))
                                    st.session_state.all_node_list = sorted(all_nodes)
                                except Exception as e:
                                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                                    st.session_state.all_node_list = []

                    if st.session_state.all_node_list:
                        # æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹
                        search_query = st.text_input(
                            "ğŸ” ãƒãƒ¼ãƒ‰æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰",
                            placeholder="ä¾‹: æ¡ƒå¤ªéƒ",
                            help="æ¤œç´¢ã—ãŸãƒãƒ¼ãƒ‰ã¨ãã®å‘¨è¾ºã‚’è¡¨ç¤ºã—ã¾ã™"
                        )

                        if search_query:
                            # æ¤œç´¢å®Ÿè¡Œ
                            matched_nodes = [n for n in st.session_state.all_node_list
                                            if search_query.lower() in n.lower()]

                            st.caption(f"ğŸ” æ¤œç´¢çµæœ: {len(matched_nodes)}ä»¶")

                            if matched_nodes:
                                # selectboxã§1ã¤é¸æŠ
                                selected_node = st.selectbox(
                                    "ãƒãƒ¼ãƒ‰ã‚’é¸æŠ",
                                    options=[""] + matched_nodes,
                                    index=0,
                                    help="ãƒªã‚¹ãƒˆã‹ã‚‰1ã¤é¸ã‚“ã§è¿½åŠ ã—ã¦ãã ã•ã„"
                                )

                                # ãƒœã‚¿ãƒ³é…ç½®
                                col1, col2 = st.columns([1, 1])
                                with col1:
                                    if selected_node and st.button("â• ä¸­å¿ƒãƒãƒ¼ãƒ‰ã«è¿½åŠ "):
                                        if selected_node not in st.session_state.center_nodes:
                                            st.session_state.center_nodes.append(selected_node)
                                            st.rerun()
                                        else:
                                            st.warning("æ—¢ã«è¿½åŠ ã•ã‚Œã¦ã„ã¾ã™")

                                with col2:
                                    if st.session_state.center_nodes and st.button("ğŸ—‘ï¸ ãƒªã‚»ãƒƒãƒˆ"):
                                        st.session_state.center_nodes = []
                                        st.rerun()
                            else:
                                st.warning(f"ã€Œ{search_query}ã€ã«ä¸€è‡´ã™ã‚‹ãƒãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                        else:
                            st.info("ğŸ’¡ ãƒãƒ¼ãƒ‰åã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã—ã¦ãã ã•ã„")

                        # é¸æŠæ¸ˆã¿ä¸­å¿ƒãƒãƒ¼ãƒ‰è¡¨ç¤º
                        if st.session_state.center_nodes:
                            st.markdown("---")
                            st.write("**ä¸­å¿ƒãƒãƒ¼ãƒ‰:**", ", ".join(st.session_state.center_nodes))

                            # Hopæ•°é¸æŠ
                            hop_distance = st.slider(
                                "å‘¨è¾ºè¡¨ç¤ºç¯„å›²ï¼ˆHopæ•°ï¼‰",
                                min_value=1,
                                max_value=3,
                                value=2,
                                help="é¸æŠãƒãƒ¼ãƒ‰ã‹ã‚‰ä½•Hopå…ˆã¾ã§è¡¨ç¤ºã™ã‚‹ã‹"
                            )

                            # ã‚µãƒ–ã‚°ãƒ©ãƒ•å–å¾—ï¼†è¡¨ç¤º
                            if st.button("ğŸ“Š ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º", type="primary"):
                                with st.spinner("ã‚µãƒ–ã‚°ãƒ©ãƒ•å–å¾—ä¸­..."):
                                    try:
                                        subgraph_data = get_enhanced_subgraph_data(
                                            st.session_state.graph,
                                            st.session_state.center_nodes,
                                            hop_distance,
                                            limit=500
                                        )

                                        if subgraph_data:
                                            # çµ±è¨ˆæƒ…å ±
                                            unique_nodes = set()
                                            for item in subgraph_data:
                                                unique_nodes.add(item['source'])
                                                unique_nodes.add(item['target'])

                                            st.success(f"âœ… ã‚µãƒ–ã‚°ãƒ©ãƒ•å–å¾—å®Œäº†")
                                            st.info(f"ğŸ“Š è¡¨ç¤º: ãƒãƒ¼ãƒ‰ {len(unique_nodes)}å€‹ / ã‚¨ãƒƒã‚¸ {len(subgraph_data)}æœ¬")

                                            # å¯è¦–åŒ–
                                            if "Agraph" in viz_engine:
                                                result = visualize_graph_agraph(subgraph_data)
                                                if not result:
                                                    st.warning("âš ï¸ Streamlit-AgraphãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Pyvisã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                                                    html = visualize_graph_pyvis_enhanced(subgraph_data)
                                                    if html:
                                                        st.components.v1.html(html, height=700)
                                            else:
                                                html = visualize_graph_pyvis_enhanced(subgraph_data)
                                                if html:
                                                    st.components.v1.html(html, height=700)
                                                else:
                                                    st.warning("å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                                        else:
                                            st.warning("é¸æŠã—ãŸãƒãƒ¼ãƒ‰ã®ã‚µãƒ–ã‚°ãƒ©ãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                                    except Exception as e:
                                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                                        import traceback
                                        st.code(traceback.format_exc())
                        else:
                            st.info("ğŸ‘† æ¤œç´¢ã—ã¦ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
                    else:
                        st.warning("ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å…ˆã«ã€Œå…¨ä½“è¡¨ç¤ºã€ã§ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚")

                else:
                    # å…¨ä½“è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜å‡¦ç†ï¼‰
                    # åˆå›ã‚°ãƒ©ãƒ•èª­ã¿è¾¼ã¿
                    if st.session_state.graph_data_cache is None:
                        if st.button("ğŸ“Š ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã‚€", type="primary"):
                            with st.spinner("ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
                                try:
                                    graph_data = get_enhanced_graph_data(st.session_state.graph, limit=max_nodes)
                                    st.session_state.graph_data_cache = graph_data
                                    st.success(f"âœ… {len(graph_data)}ä»¶ã®ã‚¨ãƒƒã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                                except Exception as e:
                                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

                    # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
                    if st.session_state.graph_data_cache:
                        try:
                            graph_data = st.session_state.graph_data_cache

                            if not graph_data:
                                st.warning("ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                            else:
                                # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
                                unique_nodes = set()
                                for item in graph_data:
                                    unique_nodes.add(item['source'])
                                    unique_nodes.add(item['target'])

                                st.info(f"ğŸ“Š è¡¨ç¤ºä¸­: ãƒãƒ¼ãƒ‰ {len(unique_nodes)}å€‹ / ã‚¨ãƒƒã‚¸ {len(graph_data)}æœ¬")

                                # å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³é¸æŠ
                                if "Agraph" in viz_engine:
                                    # Streamlit-Agraphå¯è¦–åŒ–
                                    result = visualize_graph_agraph(graph_data)
                                    if not result:
                                        # AgraphãŒå¤±æ•—ã—ãŸå ´åˆã®ã¿Pyvisã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                                        st.warning("âš ï¸ Streamlit-AgraphãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Pyvisã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
                                        html = visualize_graph_pyvis_enhanced(graph_data)
                                        if html:
                                            st.components.v1.html(html, height=700)
                                else:
                                    # Pyviså¯è¦–åŒ–
                                    html = visualize_graph_pyvis_enhanced(graph_data)
                                    if html:
                                        st.components.v1.html(html, height=700)
                                    else:
                                        st.warning("å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")

                            # ã‚°ãƒ©ãƒ•ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ãƒœã‚¿ãƒ³
                            if st.button("ğŸ”„ ã‚°ãƒ©ãƒ•ã‚’å†èª­ã¿è¾¼ã¿"):
                                st.session_state.graph_data_cache = None
                                st.session_state.all_node_list = None
                                st.rerun()

                        except Exception as e:
                            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                            import traceback
                            st.code(traceback.format_exc())

        # ãƒ¢ãƒ¼ãƒ‰2: ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
        elif display_mode == "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«":
            # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯èª­ã¿è¾¼ã¿ãƒœã‚¿ãƒ³
            if st.session_state.graph_data_cache is None:
                if st.button("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€", type="primary", key="load_data_table"):
                    with st.spinner("ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
                        try:
                            graph_data = get_enhanced_graph_data(st.session_state.graph, limit=max_nodes)
                            st.session_state.graph_data_cache = graph_data
                            st.success(f"âœ… {len(graph_data)}ä»¶ã®ã‚¨ãƒƒã‚¸ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                        except Exception as e:
                            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

            # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤º
            if st.session_state.graph_data_cache:
                try:
                    graph_data = st.session_state.graph_data_cache

                    if graph_data:
                        # NetworkXãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®å ´åˆã®ã¿ç·¨é›†æ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
                        enable_edit = (st.session_state.graph_backend == "networkx")
                        display_data_tables(
                            graph_data,
                            graph=st.session_state.graph if enable_edit else None,
                            enable_edit=enable_edit
                        )
                    else:
                        st.warning("ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ¢ãƒ¼ãƒ‰3: Cypherã‚¯ã‚¨ãƒªæ¤œç´¢
        elif display_mode == "ğŸ” Cypherã‚¯ã‚¨ãƒªæ¤œç´¢":
            st.markdown("### è‡ªç„¶è¨€èªã§ã‚°ãƒ©ãƒ•ã‚’æ¤œç´¢")
            st.info("ä¾‹: ã€Œæ¡ƒå¤ªéƒã«é–¢ã™ã‚‹ã‚°ãƒ©ãƒ•ã‚’è¦‹ãŸã„ã€ã€ŒãŠã˜ã„ã•ã‚“ã¨é–¢ä¿‚ã®ã‚ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¡¨ç¤ºã€")

            # ã‚¯ã‚¨ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            with st.expander("ğŸ“‹ ã‚¯ã‚¨ãƒªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"):
                template = st.selectbox(
                    "ã‚ˆãä½¿ã†ã‚¯ã‚¨ãƒª",
                    [
                        "ã‚«ã‚¹ã‚¿ãƒ ï¼ˆè‡ªåˆ†ã§å…¥åŠ›ï¼‰",
                        "ç‰¹å®šã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®é–¢ä¿‚ã‚’è¡¨ç¤º",
                        "æœ€ã‚‚æ¥ç¶šæ•°ãŒå¤šã„ãƒãƒ¼ãƒ‰Top10ã‚’è¡¨ç¤º",
                        "ã™ã¹ã¦ã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã‚’è¡¨ç¤º"
                    ]
                )

                if template == "ç‰¹å®šã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®é–¢ä¿‚ã‚’è¡¨ç¤º":
                    entity_name = st.text_input("ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã‚’å…¥åŠ›:", placeholder="ä¾‹: æ¡ƒå¤ªéƒ")
                    if entity_name:
                        nl_query = f"{entity_name}ã«é–¢é€£ã™ã‚‹ã™ã¹ã¦ã®é–¢ä¿‚ã‚’è¡¨ç¤º"
                    else:
                        nl_query = ""
                elif template == "æœ€ã‚‚æ¥ç¶šæ•°ãŒå¤šã„ãƒãƒ¼ãƒ‰Top10ã‚’è¡¨ç¤º":
                    nl_query = "æœ€ã‚‚æ¥ç¶šæ•°ãŒå¤šã„ãƒãƒ¼ãƒ‰Top10ã‚’è¡¨ç¤º"
                elif template == "ã™ã¹ã¦ã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã‚’è¡¨ç¤º":
                    nl_query = "ã™ã¹ã¦ã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã¨ãã®æ•°ã‚’è¡¨ç¤º"
                else:
                    nl_query = ""

            # è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªå…¥åŠ›
            user_query = st.text_area(
                "è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒª:",
                value=nl_query,
                height=100,
                placeholder="ä¾‹: æ¡ƒå¤ªéƒã«é–¢ã™ã‚‹ã‚°ãƒ©ãƒ•ã‚’è¦‹ãŸã„"
            )

            col1, col2 = st.columns([1, 4])
            with col1:
                convert_button = st.button("ğŸ”„ Cypherã«å¤‰æ›", type="primary")

            # Cypherã‚¯ã‚¨ãƒªç”Ÿæˆ
            if "generated_cypher" not in st.session_state:
                st.session_state.generated_cypher = ""

            if convert_button and user_query:
                with st.spinner("Cypherã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆä¸­..."):
                    cypher_query = natural_language_to_cypher(user_query)
                    st.session_state.generated_cypher = cypher_query

            # ç”Ÿæˆã•ã‚ŒãŸCypherã‚¯ã‚¨ãƒªè¡¨ç¤ºï¼ˆç·¨é›†å¯èƒ½ï¼‰
            if st.session_state.generated_cypher:
                st.markdown("### ğŸ“ ç”Ÿæˆã•ã‚ŒãŸCypherã‚¯ã‚¨ãƒª")
                edited_cypher = st.text_area(
                    "Cypherã‚¯ã‚¨ãƒªï¼ˆç·¨é›†å¯èƒ½ï¼‰:",
                    value=st.session_state.generated_cypher,
                    height=150,
                    key="cypher_editor"
                )

                col1, col2, col3 = st.columns([1, 1, 3])
                with col1:
                    execute_button = st.button("â–¶ï¸ å®Ÿè¡Œ", type="primary")
                with col2:
                    clear_button = st.button("ğŸ—‘ï¸ ã‚¯ãƒªã‚¢")

                if clear_button:
                    st.session_state.generated_cypher = ""
                    st.rerun()

                # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
                if execute_button and edited_cypher:
                    with st.spinner("ã‚¯ã‚¨ãƒªå®Ÿè¡Œä¸­..."):
                        result = execute_cypher_and_visualize(edited_cypher, st.session_state.graph)

                        if result:
                            st.success(f"âœ… {len(result)}ä»¶ã®çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")

                            # çµæœã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                            st.markdown("### ğŸ“Š ã‚¯ã‚¨ãƒªçµæœ")
                            import pandas as pd
                            df = pd.DataFrame(result)
                            st.dataframe(df, width='stretch')

                            # å¯è¦–åŒ–ï¼ˆsource, relation, targetãŒã‚ã‚‹å ´åˆï¼‰
                            if len(result) > 0 and 'source' in result[0] and 'target' in result[0] and 'relation' in result[0]:
                                st.markdown("### ğŸ•¸ï¸ ã‚°ãƒ©ãƒ•å¯è¦–åŒ–")

                                viz_choice = st.radio(
                                    "å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³",
                                    ["Pyvis", "Streamlit-Agraph"],
                                    horizontal=True,
                                    key="cypher_viz_engine"
                                )

                                if "Pyvis" in viz_choice:
                                    html = visualize_graph_pyvis_enhanced(result)
                                    if html:
                                        st.components.v1.html(html, height=700)
                                else:
                                    viz_result = visualize_graph_agraph(result)
                                    if not viz_result:
                                        st.warning("âš ï¸ Agraphã§è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚Pyvisã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
                                        html = visualize_graph_pyvis_enhanced(result)
                                        if html:
                                            st.components.v1.html(html, height=700)

    else:
        st.info("ã¾ãšRAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„")

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("**Graph-RAG Demo** | Powered by LangChain, Neo4j & PGVector")
