"""
Graph-RAG with LLMGraphTransformer & LCEL (stable)
=================================================
æœ€æ–°ç‰ˆã® LangChain API å¤‰å‹•ã«è¿½å¾“ã—ã€ImportError / TypeError ã‚’ã™ã¹ã¦æ½°ã—ãŸå®‰å®šç‰ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

## ä¸»ãªä¿®æ­£ç‚¹
1. **ParentDocumentRetriever ãŒè¦‹ã¤ã‹ã‚‰ãªã„ç’°å¢ƒ**ã§ã‚‚å‹•ãã‚ˆã†ã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—æ™‚ã¯ `vector_store.as_retriever()` ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
2. **SemanticChunker** ã‹ã‚‰å‰Šé™¤ã•ã‚ŒãŸ `chunk_size` å¼•æ•°ã‚’æ’é™¤ã€‚
3. **LLMGraphTransformer** ã®ãƒ¡ã‚½ãƒƒãƒ‰åã‚’ `convert_to_graph_documents()` ã«çµ±ä¸€ã€‚
4. ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã®é‡è¤‡ã‚’å‰Šé™¤ã—ã€å…¨ä½“ã‚’æ•´å½¢ã€‚

ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
--------------
```bash
pip install langchain langchain-openai langchain-community langchain-postgres \
            langchain-experimental langchain-graph-retriever neo4j tiktoken \
            python-dotenv psycopg[binary]
```
â€» `langchain` ã‚’æ˜ç¤ºçš„ã«è¿½åŠ ã—ã¾ã—ãŸã€‚ParentDocumentRetriever ãŒã“ã“ã«å­˜åœ¨ã—ã¾ã™ã€‚

`.env` ä¾‹ (Aura & RDS)
---------------------
```
OPENAI_API_KEY=sk-...

# --- Neo4j Aura ---
NEO4J_URI=neo4j+s://199e5d3d.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PW=your_neo4j_pw

# --- PGVector ---
PG_CONN=postgresql+psycopg://postgres:your_pw@localhost:5432/graph_rag
```

å®Ÿè¡Œ:
```bash
python graph_rag_lcel.py   # input.txt ã‚’åŒéšå±¤ã«é…ç½®
```
"""
from __future__ import annotations

import os
import sys
from pathlib import Path
from typing import List, Dict, Any
import hashlib

from dotenv import load_dotenv

# â”€â”€ æ—¥æœ¬èªãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from japanese_text_processor import get_japanese_processor, SUDACHI_AVAILABLE
from hybrid_retriever import HybridRetriever
from db_utils import normalize_pg_connection_string, ensure_tokenized_schema

# â”€â”€ LangChain / OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# LLM Factory for provider selection
from llm_factory import create_standard_llm
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_community.graphs import Neo4jGraph
try:
    from langchain_community.graphs.graph_document import GraphDocument  # â‰¥0.3.0
except ImportError:  # æ—§äº’æ›
    from langchain_community.graphs import GraphDocument  # type: ignore
from langchain_community.vectorstores.pgvector import PGVector

# --- GraphRetriever (å¤šæ®µãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) ---
try:
    from langchain_community.retrievers.graph import GraphRetriever  # â‰¥0.3.x
except ImportError:
    try:
        from langchain_graph_retriever import GraphRetriever
    except ImportError:
        from langchain_graph_retriever.graph_retriever import GraphRetriever

# --- ParentDocumentRetriever orãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ---
try:
    from langchain_community.retrievers.parent_document import ParentDocumentRetriever
    HAS_PARENT = True
except ImportError:
    try:
        from langchain.retrievers.parent_document import ParentDocumentRetriever  # langchain>=0.2
        HAS_PARENT = True
    except ImportError:
        HAS_PARENT = False  # fallback to simple retriever

from langchain_core.prompts import PromptTemplate
try:
    # langchain>=0.2
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import (
        RunnableParallel,
        RunnablePassthrough,
        RunnableLambda,
    )
except ImportError:  # legacy (<0.2)
    from langchain.schema.output_parser import StrOutputParser  # type: ignore
    from langchain.schema.runnable import (  # type: ignore
        RunnableParallel,
        RunnablePassthrough,
        RunnableLambda,
    )

# â”€â”€ ç’°å¢ƒå¤‰æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Graph Backendé¸æŠ
GRAPH_BACKEND = os.getenv("GRAPH_BACKEND", "networkx").lower()  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: networkx

# æ¤œç´¢çµæœæ•°è¨­å®š
RETRIEVAL_TOP_K = int(os.getenv("RETRIEVAL_TOP_K", "5"))

# Neo4j (GRAPH_BACKEND=neo4j ã®å ´åˆã®ã¿å¿…è¦)
NEO4J_URI = os.getenv("NEO4J_URI")
NEO4J_USER = os.getenv("NEO4J_USER")
NEO4J_PW = os.getenv("NEO4J_PW")

# Postgres / PGVector
PG_CONN = os.getenv("PG_CONN")
if not PG_CONN:
    raise ValueError("PG_CONN ç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™ã€‚")

# ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æ¤œè¨¼
if GRAPH_BACKEND not in ["neo4j", "networkx"]:
    raise ValueError(f"GRAPH_BACKEND ã¯ 'neo4j' ã¾ãŸã¯ 'networkx' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ç¾åœ¨: {GRAPH_BACKEND}")

if GRAPH_BACKEND == "neo4j" and not all([NEO4J_URI, NEO4J_USER, NEO4J_PW]):
    raise ValueError("GRAPH_BACKEND=neo4j ã®å ´åˆã€NEO4J_URI, NEO4J_USER, NEO4J_PW ãŒå¿…è¦ã§ã™ã€‚")

# â”€â”€ 0. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOC_PATH = "input.txt"
if not Path(DOC_PATH).is_file():
    raise FileNotFoundError(f"{DOC_PATH} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
raw_text = Path(DOC_PATH).read_text(encoding="utf-8")

# â”€â”€ 1. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² (RecursiveCharacterTextSplitter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
chunker = RecursiveCharacterTextSplitter(
    chunk_size=500,           # 500æ–‡å­—ã”ã¨ã«åˆ†å‰²
    chunk_overlap=100,        # 100æ–‡å­—ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡è„ˆä¿æŒï¼‰
    separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""],  # æ—¥æœ¬èªå¯¾å¿œ
    length_function=len
)
chunks = chunker.create_documents([raw_text])

# é‡è¤‡ãƒãƒ£ãƒ³ã‚¯ã‚’å†…å®¹ãƒãƒƒã‚·ãƒ¥ã§é™¤å»ã—ã€ãƒãƒƒã‚·ãƒ¥ã‚’IDã¨ã—ã¦ä»˜ä¸
deduped = []
seen_hashes = set()
for chunk in chunks:
    digest = hashlib.sha256(chunk.page_content.encode("utf-8")).hexdigest()
    if digest in seen_hashes:
        continue
    seen_hashes.add(digest)
    chunk.metadata["id"] = digest
    deduped.append(chunk)
chunks = deduped


# --- ãƒ™ã‚¯ãƒˆãƒ«DBã‚¯ãƒªã‚¢æ©Ÿèƒ½ï¼ˆUI/CLI/ENVã§åˆ©ç”¨ï¼‰ ---
class _DummyEmbeddings:
    """OpenAIã‚’å‘¼ã°ãšã«ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å‰Šé™¤ã ã‘è¡Œã†ãŸã‚ã®ãƒ€ãƒŸãƒ¼åŸ‹ã‚è¾¼ã¿"""

    def __init__(self, dim: int = 1536) -> None:
        self.dim = dim

    def embed_query(self, _: str):
        return [0.0] * self.dim

    def embed_documents(self, texts):
        return [[0.0] * self.dim for _ in texts]


def clear_vector_store() -> None:
    """PGVector ã®æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹"""
    store = PGVector(
        connection_string=PG_CONN,
        embedding_function=_DummyEmbeddings(),
        collection_name="graphrag",
        embedding_length=1536,
    )
    store.delete_collection()
    print("âœ… Vector store collection 'graphrag' ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")


def maybe_handle_control_mode() -> None:
    """UIãƒ¡ãƒ‹ãƒ¥ãƒ¼/CLIå¼•æ•°/ç’°å¢ƒå¤‰æ•°ã§ã‚¯ãƒªã‚¢æŒ‡ç¤ºãŒã‚ã‚Œã°å³å®Ÿè¡Œã—ã¦çµ‚äº†"""
    args = [a.lower() for a in sys.argv[1:]]
    env_clear = os.getenv("CLEAR_VECTOR_STORE") == "1"
    use_ui = os.getenv("GRAPHRAG_UI") == "1" or any(a in {"ui", "menu", "manage"} for a in args)
    wants_clear = env_clear or any(a.strip("-") in {"clear", "reset", "cleanup"} for a in args)

    if use_ui:
        print("[GraphRAG ç®¡ç†ãƒ¡ãƒ‹ãƒ¥ãƒ¼]")
        print("  1) Vector store ã‚’å‰Šé™¤ã™ã‚‹")
        print("  2) é€šå¸¸å®Ÿè¡Œã™ã‚‹")
        print("  3) ä½•ã‚‚ã—ãªã„ã§çµ‚äº†")
        choice = input("é¸æŠç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ [1/2/3]: ").strip()
        if choice == "1":
            clear_vector_store()
            sys.exit(0)
        if choice == "3":
            sys.exit(0)
        # choice == "2" ã¯ãã®ã¾ã¾ç¶šè¡Œ

    if wants_clear:
        clear_vector_store()
        sys.exit(0)


maybe_handle_control_mode()

# â”€â”€ 2. LLMGraphTransformer ã§ GraphDocument åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm = create_standard_llm(model="gpt-4o-mini", temperature=0)
transformer = LLMGraphTransformer(llm=llm)
graph_docs: List[GraphDocument] = transformer.convert_to_graph_documents(chunks)

# â”€â”€ 3. ã‚°ãƒ©ãƒ•ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ãƒ­ãƒ¼ãƒ‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if GRAPH_BACKEND == "neo4j":
    graph = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PW)
    graph.add_graph_documents(graph_docs, include_source=True)
    print(f"âœ… Neo4jã«ã‚°ãƒ©ãƒ•ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ (URI: {NEO4J_URI})")
else:  # networkx
    from networkx_graph import NetworkXGraph
    graph = NetworkXGraph(storage_path="graph.pkl", auto_save=True)
    graph.add_graph_documents(graph_docs, include_source=True)
    print(f"âœ… NetworkXã«ã‚°ãƒ©ãƒ•ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ (ä¿å­˜å…ˆ: graph.pkl)")

# â”€â”€ 3.5. æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
japanese_processor = get_japanese_processor()
enable_japanese_search = os.getenv("ENABLE_JAPANESE_SEARCH", "true").lower() == "true"

if japanese_processor and enable_japanese_search:
    ensure_tokenized_schema(PG_CONN)
    print("ğŸ“ æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ä¸­...")
    for chunk in chunks:
        try:
            tokenized = japanese_processor.tokenize(chunk.page_content)
            chunk.metadata['tokenized_content'] = tokenized
        except Exception as e:
            print(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {e}")
            chunk.metadata['tokenized_content'] = None

# â”€â”€ 4. PGVector ã«ãƒãƒ£ãƒ³ã‚¯ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vector_store = PGVector.from_documents(
    chunks,
    embeddings,
    connection_string=PG_CONN,
    collection_name="graphrag",
    pre_delete_collection=True,  # å†å®Ÿè¡Œæ™‚ã«æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¦é‡è¤‡ã‚’é˜²æ­¢
    ids=[c.metadata["id"] for c in chunks],  # åŒä¸€IDã®å†ç™»éŒ²ã‚’é˜²ã
)

# ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’DBã«åæ˜ 
if japanese_processor and enable_japanese_search:
    try:
        import psycopg
        raw_pg_conn = normalize_pg_connection_string(PG_CONN)
        with psycopg.connect(raw_pg_conn) as conn:
            with conn.cursor() as cur:
                for chunk in chunks:
                    tokenized = chunk.metadata.get('tokenized_content')
                    if tokenized:
                        cur.execute("""
                            UPDATE langchain_pg_embedding
                            SET tokenized_content = %s
                            WHERE cmetadata->>'id' = %s
                        """, (tokenized, chunk.metadata['id']))
            conn.commit()
        print("âœ… æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ‡ãƒ¼ã‚¿ã®DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# â”€â”€ 5. Retriever æ§‹ç¯‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if GRAPH_BACKEND == "neo4j":
    graph_retriever = GraphRetriever(graph=graph, k=RETRIEVAL_TOP_K, search_type="cypher")
else:  # networkx
    from networkx_graph import NetworkXGraphRetriever
    # NetworkXã®å ´åˆã¯ã€ã‚ˆã‚Šå¤šãã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å–å¾—ã—ã¦LLMã§å‡¦ç†
    graph_retriever = NetworkXGraphRetriever(graph=graph, k=RETRIEVAL_TOP_K * 3, llm=llm)

if HAS_PARENT:
    vector_retriever = ParentDocumentRetriever(vector_store, search_kwargs={"k": RETRIEVAL_TOP_K})
else:
    vector_retriever = vector_store.as_retriever(search_kwargs={"k": RETRIEVAL_TOP_K})

# â”€â”€ 6. LCEL ãƒã‚§ã‚¤ãƒ³å®šç¾© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
graph_run = graph_retriever.as_runnable()
vector_run = vector_retriever.as_runnable()


def merge_ctx(data: Dict[str, Any]) -> Dict[str, Any]:
    """Graph/Vector ã®çµæœã‚’ 1 ã¤ã® context æ–‡å­—åˆ—ã¸æ•´å½¢"""
    triples = data["graph"]
    docs = data["docs"]
    graph_lines = [
        f"{t.get('start') or t.get('subject')} -[{t.get('predicate') or t.get('type')}]â†’ {t.get('end') or t.get('object')}"
        for t in triples
    ]
    context = (
        "<GRAPH_CONTEXT>\n" + "\n".join(graph_lines) + "\n</GRAPH_CONTEXT>\n\n" +
        "<DOCUMENT_CONTEXT>\n" + "\n---\n".join(d.page_content for d in docs) + "\n</DOCUMENT_CONTEXT>"
    )
    return {"context": context, "question": data["question"]}

prompt = PromptTemplate.from_template(
    """ã‚ãªãŸã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚\nè³ªå•: {question}\n\n{context}\n\n---\nä¸Šè¨˜æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã«ã€æ—¥æœ¬èªã§ç¶²ç¾…çš„ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚å¿…è¦ã«å¿œã˜ã¦é–¢é€£ãƒãƒ£ãƒ³ã‚¯ã‚„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚"""
)

chain = (
    {"question": RunnablePassthrough()}
    | RunnableParallel({"graph": graph_run, "docs": vector_run})
    | RunnableLambda(merge_ctx)
    | prompt
    | create_standard_llm(model="gpt-4o-mini", temperature=0)
    | StrOutputParser()
)

# â”€â”€ 7. å¯¾è©±ãƒ«ãƒ¼ãƒ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print(f"Graph-RAG LCEL ãƒ‡ãƒ¢ (Backend: {GRAPH_BACKEND.upper()})")
    print("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (exit ã§çµ‚äº†)ã€‚")
    while True:
        q = input("\nè³ªå•> ").strip()
        if q.lower() in {"exit", "quit", "q"}:
            break
        print("\n--- å›ç­” ---")
        try:
            print(chain.invoke(q))
        except Exception as e:
            print(f"[Error] {e}")
